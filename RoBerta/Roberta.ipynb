{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roberta.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XXhsEvzIjlZP",
        "vxm8QiFjiWRg"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4300a7567e6044ef99cede8f69295912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50355d4373c04aae8dea0eb07ae539d0",
              "IPY_MODEL_70baae3535c24251b062e8c240754a6c",
              "IPY_MODEL_0280d7fa237441669cd9b09ed14ba814"
            ],
            "layout": "IPY_MODEL_0b99669061e942628a4c043cc2cfe3fe"
          }
        },
        "50355d4373c04aae8dea0eb07ae539d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1f72527adb649f6b676ca1e6a214ad5",
            "placeholder": "​",
            "style": "IPY_MODEL_5810a90db0bf4fd4a389a73056e0c0e9",
            "value": "Downloading: 100%"
          }
        },
        "70baae3535c24251b062e8c240754a6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f246208aafd74c5aa6c49b3e2d353855",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7fc7e446415436e897913e74045751d",
            "value": 898823
          }
        },
        "0280d7fa237441669cd9b09ed14ba814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dfafafd2b2e495eaf425e2349b66af2",
            "placeholder": "​",
            "style": "IPY_MODEL_3da428bccf624a838fa9f3988989eb3f",
            "value": " 878k/878k [00:01&lt;00:00, 1.10MB/s]"
          }
        },
        "0b99669061e942628a4c043cc2cfe3fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1f72527adb649f6b676ca1e6a214ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5810a90db0bf4fd4a389a73056e0c0e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f246208aafd74c5aa6c49b3e2d353855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7fc7e446415436e897913e74045751d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dfafafd2b2e495eaf425e2349b66af2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3da428bccf624a838fa9f3988989eb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7961a2057ff74d449791dd8e5b0fc5c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f4e6cd8af194206b2e3f0d3296dd605",
              "IPY_MODEL_ea6a488b03d44ddabb3092dfcac67e64",
              "IPY_MODEL_85f515666b00486ca51ac079d85d9ad1"
            ],
            "layout": "IPY_MODEL_48dea6c14d2e448bacf264d5b31b2051"
          }
        },
        "8f4e6cd8af194206b2e3f0d3296dd605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc46f06615d42e1bc18e37140915945",
            "placeholder": "​",
            "style": "IPY_MODEL_3cf091c799fd4d7fad810268c90093b8",
            "value": "Downloading: 100%"
          }
        },
        "ea6a488b03d44ddabb3092dfcac67e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13ba322aff2d4dafb08fe951b0924c43",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3de4d6a57984bb287bbfce14999948f",
            "value": 456318
          }
        },
        "85f515666b00486ca51ac079d85d9ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_886560bfcda54e9097b0b1c2156c21bb",
            "placeholder": "​",
            "style": "IPY_MODEL_216a3dd09a574209b2c4cb715f98b783",
            "value": " 446k/446k [00:01&lt;00:00, 518kB/s]"
          }
        },
        "48dea6c14d2e448bacf264d5b31b2051": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fc46f06615d42e1bc18e37140915945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf091c799fd4d7fad810268c90093b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13ba322aff2d4dafb08fe951b0924c43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3de4d6a57984bb287bbfce14999948f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "886560bfcda54e9097b0b1c2156c21bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "216a3dd09a574209b2c4cb715f98b783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5a102641fb04d318ff015259a30b82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24ea5d63a37d47af8b0348f9b5053b3a",
              "IPY_MODEL_d433400f596c49aea23dbcd806c251c7",
              "IPY_MODEL_b3f000f3346f4796bcbe30c2672179d3"
            ],
            "layout": "IPY_MODEL_25709262d4394cd08da243d6e22b2d34"
          }
        },
        "24ea5d63a37d47af8b0348f9b5053b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90546a14afee4c62b58480c60b38df80",
            "placeholder": "​",
            "style": "IPY_MODEL_56b66e6b9b9345d7a827267175f33df1",
            "value": "Downloading: 100%"
          }
        },
        "d433400f596c49aea23dbcd806c251c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9511d4f1fd8441b3a2d36e3b0db1711c",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71bf2722a7b345368b65ed465bdc83af",
            "value": 481
          }
        },
        "b3f000f3346f4796bcbe30c2672179d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73b1642c4417446dbe867e1f609c61b3",
            "placeholder": "​",
            "style": "IPY_MODEL_57e026e067d44905b0d6e4fd63f035f1",
            "value": " 481/481 [00:00&lt;00:00, 5.06kB/s]"
          }
        },
        "25709262d4394cd08da243d6e22b2d34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90546a14afee4c62b58480c60b38df80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56b66e6b9b9345d7a827267175f33df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9511d4f1fd8441b3a2d36e3b0db1711c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71bf2722a7b345368b65ed465bdc83af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73b1642c4417446dbe867e1f609c61b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57e026e067d44905b0d6e4fd63f035f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "507d1884e9fe464c81efaf50516d0b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a87a665b9454da4b2e862367c88a53e",
              "IPY_MODEL_bee2a6aa1aa0479197c40e3b14db61cd",
              "IPY_MODEL_506c6deafff44a3da063651929e28422"
            ],
            "layout": "IPY_MODEL_b816d7ec868842e5a6aafdb1e15607a6"
          }
        },
        "5a87a665b9454da4b2e862367c88a53e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_479d1b31d5c34fc68c1c98fdfe6ae484",
            "placeholder": "​",
            "style": "IPY_MODEL_a06de19af9f74c259c96ab63464bd95e",
            "value": "Downloading: 100%"
          }
        },
        "bee2a6aa1aa0479197c40e3b14db61cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc8c402a746a43829aded077aeb1ab99",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5f8881ae8da41cc97ce46422c556e1f",
            "value": 501200538
          }
        },
        "506c6deafff44a3da063651929e28422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b653a34c462e44a8af8846d62687007e",
            "placeholder": "​",
            "style": "IPY_MODEL_3d0492b16e204b8da0e91b345bff3921",
            "value": " 478M/478M [00:15&lt;00:00, 32.8MB/s]"
          }
        },
        "b816d7ec868842e5a6aafdb1e15607a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "479d1b31d5c34fc68c1c98fdfe6ae484": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a06de19af9f74c259c96ab63464bd95e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc8c402a746a43829aded077aeb1ab99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5f8881ae8da41cc97ce46422c556e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b653a34c462e44a8af8846d62687007e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d0492b16e204b8da0e91b345bff3921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXhsEvzIjlZP"
      },
      "source": [
        "#Fast OMP Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM6DBfeWjtT6"
      },
      "outputs": [],
      "source": [
        "%load_ext Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wQlHgEOj2lW"
      },
      "outputs": [],
      "source": [
        "%%cython\n",
        "\n",
        "\"\"\"Cython allows us to call lower level c-code instead of using Python. It can be a surprisingly big speedup!\"\"\"\n",
        "import cython\n",
        "from scipy.linalg.cython_blas cimport idamax, isamax, daxpy, dgemv, dtrmv, dcopy\n",
        "from scipy.linalg.cython_lapack cimport dposv, dppsv, sppsv\n",
        "\n",
        "ctypedef fused proj_t:\n",
        "    double\n",
        "    float\n",
        "\n",
        "@cython.boundscheck(False)\n",
        "@cython.wraparound(False)\n",
        "cpdef void ppsv(proj_t[:, :] As,\n",
        "           proj_t[:, :, :] ys) nogil:\n",
        "    # Works not for strided array I think. And please do not give a negative-stride array.\n",
        "    cdef Py_ssize_t B = ys.shape[0]  # Batch size\n",
        "    cdef int N = ys.shape[1]\n",
        "    cdef int nrhs = ys.shape[2]\n",
        "    cdef int info = 0  # Just discard any error signals ;)\n",
        "    cdef char uplo = 85 # The letter 'U', since we store the lower triangle and fortran sees As.T.\n",
        "    # cdef int ldb = ys[0].strides[0] // sizeof(double)\n",
        "\n",
        "    for i from 0 <= i < B:\n",
        "        if proj_t is double:  # One C-function is created for each of these specializations! :) (see argmax_blast.__signatures__)\n",
        "            dppsv(&uplo, &N, &nrhs, &As[i, 0], &ys[i, 0, 0], &N, &info)\n",
        "        elif proj_t is float:\n",
        "            sppsv(&uplo, &N, &nrhs, &As[i, 0], &ys[i, 0, 0], &N, &info)\n",
        "\n",
        "\n",
        "@cython.boundscheck(False)\n",
        "@cython.wraparound(False)\n",
        "cpdef void argmax_blast(proj_t[:, :] projections,\n",
        "                 long long[:] output) nogil:\n",
        "    # TODO: Numpy has its own indexing data-type - this may be a more appropriate output, and may even be faster.\n",
        "    # http://conference.scipy.org/static/wiki/seljebotn_cython.pdf\n",
        "    # https://apprize.best/python/cython/3.html\n",
        "    cdef Py_ssize_t B = projections.shape[0]\n",
        "    cdef int N = projections.shape[1]\n",
        "    cdef int incx = projections.strides[1] // sizeof(proj_t)  # Stride between elements.\n",
        "    cdef Py_ssize_t i\n",
        "    for i from 0 <= i < B:\n",
        "        if proj_t is double:\n",
        "            output[i] = idamax(&N, &projections[i, 0], &incx) - 1\n",
        "        elif proj_t is float:\n",
        "            output[i] = isamax(&N, &projections[i, 0], &incx) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYR0qH2Xj73q"
      },
      "outputs": [],
      "source": [
        "\"\"\"This cell contains the code we've implemented. You should be able to call each function directly, or alternatively, see our example calls below\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.utils\n",
        "import torch.utils.data\n",
        "from sklearn.datasets import make_sparse_coded_signal\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "from contextlib import contextmanager\n",
        "from timeit import default_timer\n",
        "# from test_omp import omp_naive\n",
        "# from test import *  # FIXME: better name\n",
        "# from line_profiler import line_profiler\n",
        "\n",
        "n_components, n_features = 100, 100\n",
        "n_nonzero_coefs = 17\n",
        "n_samples = 50\n",
        "\n",
        "@contextmanager\n",
        "def elapsed_timer():\n",
        "    # https://stackoverflow.com/questions/7370801/how-to-measure-elapsed-time-in-python\n",
        "    start = default_timer()\n",
        "    elapser = lambda: default_timer() - start\n",
        "    yield lambda: elapser()\n",
        "    end = default_timer()\n",
        "    elapser = lambda: end-start\n",
        "\n",
        "\n",
        "def run_omp(X, y, n_nonzero_coefs, precompute=True, tol=0.0, normalize=False, fit_intercept=False, alg='naive'):\n",
        "    if not isinstance(X, torch.Tensor):\n",
        "        X = torch.as_tensor(X)\n",
        "        y = torch.as_tensor(y)\n",
        "\n",
        "    # We can either return sets, (sets, solutions), or xests\n",
        "    # These are all equivalent, but are simply more and more dense representations.\n",
        "    # Given sets and X and y one can (re-)construct xests. The second is just a sparse vector repr.\n",
        "\n",
        "    # https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/linear_model/_omp.py#L690\n",
        "    if fit_intercept or normalize:\n",
        "        X = X.clone()\n",
        "        assert not isinstance(precompute, torch.Tensor), \"If user pre-computes XTX they can also pre-normalize X\" \\\n",
        "                                                         \" as well, so normalize and fit_intercept must be set false.\"\n",
        "\n",
        "    if fit_intercept:\n",
        "        X = X - X.mean(0)\n",
        "        y = y - y.mean(1)[:, None]\n",
        "\n",
        "    # To keep a good condition number on X, especially with Cholesky compared to LU factorization,\n",
        "    # we should probably always normalize it (OMP is invariant anyways)\n",
        "    if normalize is True:  # User can also just optionally supply pre-computed norms.\n",
        "        normalize = (X * X).sum(0).sqrt()\n",
        "        X /= normalize[None, :]\n",
        "\n",
        "    if precompute is True or alg == 'v0':\n",
        "        precompute = X.T @ X\n",
        "\n",
        "    # If n_nonzero_coefs is equal to M, one should just return lstsq\n",
        "    if alg == 'naive':\n",
        "        sets, solutions, lengths = omp_naive(X, y, n_nonzero_coefs=n_nonzero_coefs, XTX=precompute, tol=tol)\n",
        "    elif alg == 'v0':\n",
        "        sets, solutions, lengths = omp_v0(X, y, n_nonzero_coefs=n_nonzero_coefs, XTX=precompute, tol=tol)\n",
        "\n",
        "\n",
        "    solutions = solutions.squeeze(-1)\n",
        "    if normalize is not False:\n",
        "        solutions /= normalize[sets]\n",
        "\n",
        "    xests = y.new_zeros(y.shape[0], X.shape[1])\n",
        "    if lengths is None:\n",
        "        xests[torch.arange(y.shape[0], dtype=sets.dtype, device=sets.device)[:, None], sets] = solutions\n",
        "    else:\n",
        "        for i in range(y.shape[0]):\n",
        "            # xests[i].scatter_(-1, sets[i, :lengths[i]], solutions[i, :lengths[i]])\n",
        "            xests[i, sets[i, :lengths[i]]] = solutions[i, :lengths[i]]\n",
        "\n",
        "    return xests\n",
        "\n",
        "def batch_mm(matrix, matrix_batch, return_contiguous=True):\n",
        "    \"\"\"\n",
        "    :param matrix: Sparse or dense matrix, size (m, n).\n",
        "    :param matrix_batch: Batched dense matrices, size (b, n, k).\n",
        "    :return: The batched matrix-matrix product, size (m, n) x (b, n, k) = (b, m, k).\n",
        "    \"\"\"\n",
        "    # One dgemm is faster than many dgemv.\n",
        "    # From https://github.com/pytorch/pytorch/issues/14489#issuecomment-607730242\n",
        "    batch_size = matrix_batch.shape[0]\n",
        "    # Stack the vector batch into columns. (b, n, k) -> (n, b, k) -> (n, b*k)\n",
        "    vectors = matrix_batch.transpose([1, 0, 2]).reshape(matrix.shape[1], -1)\n",
        "\n",
        "    # A matrix-matrix product is a batched matrix-vector product of the columns.\n",
        "    # And then reverse the reshaping. (m, n) x (n, b*k) = (m, b*k) -> (m, b, k) -> (b, m, k)\n",
        "    if return_contiguous:\n",
        "        result = np.empty_like(matrix_batch, shape=(batch_size, matrix.shape[0], matrix_batch.shape[2]))\n",
        "        np.matmul(matrix, vectors, out=result.transpose([1, 0, 2]).reshape(matrix.shape[0], -1))\n",
        "    else:\n",
        "        result = (matrix @ vectors).reshape(matrix.shape[0], batch_size, -1).transpose([1, 0, 2])\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def innerp(x, y=None, out=None):\n",
        "    if y is None:\n",
        "        y = x\n",
        "    if out is not None:\n",
        "        out = out[:, None, None]  # Add space for two singleton dimensions.\n",
        "    return torch.matmul(x[..., None, :], y[..., :, None], out=out)[..., 0, 0]\n",
        "\n",
        "def cholesky_solve(ATA, ATy):\n",
        "    if ATA.dtype == torch.half or ATy.dtype == torch.half:\n",
        "        return ATy.to(torch.float).cholesky_solve(torch.cholesky(ATA.to(torch.float))).to(ATy.dtype)\n",
        "    return ATy.cholesky_solve(torch.cholesky(ATA)).to(ATy.dtype)\n",
        "\n",
        "\n",
        "def omp_naive(X, y, n_nonzero_coefs, tol=None, XTX=None):\n",
        "    on_cpu = not (y.is_cuda or y.dtype == torch.half)\n",
        "    # torch.cuda.synchronize()\n",
        "    # Given X as an MxN array and y as an BxN array, do omp to approximately solve Xb=y\n",
        "\n",
        "    # Base variables\n",
        "    XT = X.contiguous().t()  # Store XT in fortran-order.\n",
        "    y = y.contiguous()\n",
        "    r = y.clone()\n",
        "\n",
        "    sets = y.new_zeros((n_nonzero_coefs, y.shape[0]), dtype=torch.long).t()\n",
        "    if tol:\n",
        "        result_sets = sets.new_zeros(y.shape[0], n_nonzero_coefs)\n",
        "        result_lengths = sets.new_zeros(y.shape[0])\n",
        "        result_solutions = y.new_zeros((y.shape[0], n_nonzero_coefs, 1))\n",
        "        original_indices = torch.arange(y.shape[0], dtype=sets.dtype, device=sets.device)\n",
        "\n",
        "    # Trade b*k^2+bk+bkM = O(bkM) memory for much less compute time. (This has to be done anyways since we are batching,\n",
        "    # otherwise one could just permute columns of X in-place as in https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/linear_model/_omp.py#L28 )\n",
        "    ATs = y.new_zeros(r.shape[0], n_nonzero_coefs, X.shape[0])\n",
        "    ATys = y.new_zeros(r.shape[0], n_nonzero_coefs, 1)\n",
        "    ATAs = torch.eye(n_nonzero_coefs, dtype=y.dtype, device=y.device)[None].repeat(r.shape[0], 1, 1)\n",
        "    if on_cpu:\n",
        "        # For CPU it is faster to use a packed representation of the lower triangle in ATA.\n",
        "        tri_idx = torch.tril_indices(n_nonzero_coefs, n_nonzero_coefs, device=sets.device, dtype=sets.dtype)\n",
        "        ATAs = ATAs[:, tri_idx[0], tri_idx[1]]\n",
        "\n",
        "    solutions = y.new_zeros((r.shape[0], 0))\n",
        "\n",
        "    for k in range(n_nonzero_coefs+bool(tol)):\n",
        "        # STOPPING CRITERIA\n",
        "        if tol:\n",
        "            problems_done = innerp(r) <= tol\n",
        "            if k == n_nonzero_coefs:\n",
        "                problems_done[:] = True\n",
        "\n",
        "            if problems_done.any():\n",
        "                remaining = ~problems_done\n",
        "\n",
        "                orig_idxs = original_indices[problems_done]\n",
        "                result_sets[orig_idxs, :k] = sets[problems_done, :k]\n",
        "                result_solutions[orig_idxs, :k] = solutions[problems_done]\n",
        "                result_lengths[orig_idxs] = k\n",
        "                original_indices = original_indices[remaining]\n",
        "\n",
        "                # original_indices = original_indices[remaining]\n",
        "                ATs = ATs[remaining]\n",
        "                ATys = ATys[remaining]\n",
        "                ATAs = ATAs[remaining]\n",
        "                sets = sets[remaining]\n",
        "                y = y[remaining]\n",
        "                r = r[remaining]\n",
        "                if problems_done.all():\n",
        "                    return result_sets, result_solutions, result_lengths\n",
        "        # GET PROJECTIONS AND INDICES TO ADD\n",
        "        if on_cpu:\n",
        "            projections = batch_mm(XT.numpy(), r[:, :, None].numpy())\n",
        "            argmax_blast(projections.squeeze(-1), sets[:, k].numpy())\n",
        "        else:\n",
        "            projections = XT @ r[:, :, None]\n",
        "            sets[:, k] = projections.abs().sum(-1).argmax(-1)  # Sum is just a squeeze, but would be relevant in SOMP.\n",
        "\n",
        "        # UPDATE AT\n",
        "        AT = ATs[:, :k + 1, :]\n",
        "        updateA = XT[sets[:, k], :]\n",
        "        AT[:, k, :] = updateA\n",
        "\n",
        "        # UPDATE ATy based on AT\n",
        "        ATy = ATys[:, :k + 1]\n",
        "        innerp(updateA, y, out=ATy[:, k, 0])\n",
        "\n",
        "        # UPDATE ATA based on AT or precomputed XTX.\n",
        "        if on_cpu:\n",
        "            packed_idx = k * (k - 1) // 2\n",
        "            if XTX is not None:  # Update based on precomputed XTX.\n",
        "                ATAs.t()[k + packed_idx:packed_idx + 2 * k + 1, :].t().numpy()[:] = XTX[sets[:, k, None], sets[:, :k + 1]]\n",
        "            else:\n",
        "                np.matmul(AT[:, :k + 1, :].numpy(), updateA[:, :, None].numpy(),\n",
        "                          out=ATAs.t()[k + packed_idx:packed_idx + 2 * k + 1, :].t()[:, :, None].numpy())\n",
        "        else:\n",
        "            ATA = ATAs[:, :k + 1, :k + 1]\n",
        "            if XTX is not None:\n",
        "                ATA[:, k, :k + 1] = XTX[sets[:, k, None], sets[:, :k + 1]]\n",
        "            else:\n",
        "                # Update ATAs by adding the new column of inner products.\n",
        "                torch.bmm(AT[:, :k + 1, :], updateA[:, :, None], out=ATA[:, k, :k + 1, None])\n",
        "\n",
        "        # SOLVE ATAx = ATy.\n",
        "        if on_cpu:\n",
        "            solutions = ATy.permute(0, 2, 1).clone().permute(0, 2, 1)  # Get a copy.\n",
        "            ppsv(ATAs.t()[:packed_idx + 2 * k + 1, :].t().contiguous().numpy(), solutions.numpy())\n",
        "        else:\n",
        "            ATA[:, :k, k] = ATA[:, k, :k]  # Copy lower triangle to upper triangle.\n",
        "            solutions = cholesky_solve(ATA, ATy)\n",
        "\n",
        "        # FINALLY, GET NEW RESIDUAL r=y-Ax\n",
        "        if on_cpu:\n",
        "            np.subtract(y.numpy(), (AT.permute(0, 2, 1).numpy() @ solutions.numpy()).squeeze(-1), out=r.numpy())\n",
        "        else:\n",
        "            torch.baddbmm(y[:, :, None], AT.permute(0, 2, 1), solutions, beta=-1, out=r[:, :, None])\n",
        "\n",
        "    return sets, solutions, None\n",
        "\n",
        "def omp_v0(X, y, XTX, n_nonzero_coefs=None, tol=None, inverse_cholesky=True):\n",
        "    B = y.shape[0]\n",
        "    normr2 = innerp(y)  # Norm squared of residual.\n",
        "    projections = (X.transpose(1, 0) @ y[:, :, None]).squeeze(-1)\n",
        "    sets = y.new_zeros(n_nonzero_coefs, B, dtype=torch.int64)\n",
        "\n",
        "    if inverse_cholesky:\n",
        "        # Doing the inverse-cholesky iteratively uses more memory,\n",
        "        # but takes less time than waiting till solving the problem in the end it seems.\n",
        "        # (Since F is triangular it could be __even faster__ to multiply, prob. not on GPU tho.)\n",
        "        F = torch.eye(n_nonzero_coefs, dtype=y.dtype, device=y.device).repeat(B, 1, 1)\n",
        "        a_F = y.new_zeros(n_nonzero_coefs, B, 1)\n",
        "\n",
        "    D_mybest = y.new_empty(B, n_nonzero_coefs, XTX.shape[0])\n",
        "    temp_F_k_k = y.new_ones((B, 1))\n",
        "\n",
        "    if tol:\n",
        "        result_lengths = sets.new_zeros(y.shape[0])\n",
        "        result_solutions = y.new_zeros((y.shape[0], n_nonzero_coefs, 1))\n",
        "        finished_problems = sets.new_zeros(y.shape[0], dtype=torch.bool)\n",
        "\n",
        "    for k in range(n_nonzero_coefs+bool(tol)):\n",
        "        # STOPPING CRITERIA\n",
        "        if tol:\n",
        "            problems_done = normr2 <= tol\n",
        "            if k == n_nonzero_coefs:\n",
        "                problems_done[:] = True\n",
        "\n",
        "            if problems_done.any():\n",
        "                new_problems_done = problems_done & ~finished_problems\n",
        "                finished_problems.logical_or_(problems_done)\n",
        "                result_lengths[new_problems_done] = k\n",
        "                if inverse_cholesky:\n",
        "                    result_solutions[new_problems_done, :k] = F[new_problems_done, :k, :k].permute(0, 2, 1) @ a_F[:k, new_problems_done].permute(1, 0, 2)\n",
        "                else:\n",
        "                    assert False, \"inverse_cholesky=False with tol != None is not handled yet\"\n",
        "                if problems_done.all():\n",
        "                    return sets.t(), result_solutions, result_lengths\n",
        "\n",
        "        sets[k] = projections.abs().argmax(1)\n",
        "        # D_mybest[:, k, :] = XTX[gamma[k], :]  # Same line as below, but significantly slower. (prob. due to the intermediate array creation)\n",
        "        torch.gather(XTX, 0, sets[k, :, None].expand(-1, XTX.shape[1]), out=D_mybest[:, k, :])\n",
        "        if k:\n",
        "            D_mybest_maxindices = D_mybest.permute(0, 2, 1)[torch.arange(D_mybest.shape[0], dtype=sets.dtype, device=sets.device), sets[k], :k]\n",
        "            torch.rsqrt(1 - innerp(D_mybest_maxindices),\n",
        "                        out=temp_F_k_k[:, 0])  # torch.exp(-1/2 * torch.log1p(-inp), temp_F_k_k[:, 0])\n",
        "            D_mybest_maxindices *= -temp_F_k_k  # minimal operations, exploit linearity\n",
        "            D_mybest[:, k, :] *= temp_F_k_k\n",
        "            D_mybest[:, k, :, None].baddbmm_(D_mybest[:, :k, :].permute(0, 2, 1), D_mybest_maxindices[:, :, None])\n",
        "\n",
        "\n",
        "        temp_a_F = temp_F_k_k * torch.gather(projections, 1, sets[k, :, None])\n",
        "        normr2 -= (temp_a_F * temp_a_F).squeeze(-1)\n",
        "        projections -= temp_a_F * D_mybest[:, k, :]\n",
        "        if inverse_cholesky:\n",
        "            a_F[k] = temp_a_F\n",
        "            if k:  # Could maybe get a speedup from triangular mat mul kernel.\n",
        "                torch.bmm(D_mybest_maxindices[:, None, :], F[:, :k, :], out=F[:, k, None, :])\n",
        "                F[:, k, k] = temp_F_k_k[..., 0]\n",
        "    else: # FIXME: else branch will not execute if n_nonzero_coefs=0, so solutions is undefined.\n",
        "        # Normal exit, used when tolerance=None.\n",
        "        if inverse_cholesky:\n",
        "            solutions = F.permute(0, 2, 1) @ a_F.squeeze(-1).transpose(1, 0)[:, :, None]\n",
        "        else:\n",
        "            # Solving the problem in the end without using inverse Cholesky.\n",
        "            AT = X.T[sets.T]\n",
        "            solutions = cholesky_solve(AT @ AT.permute(0, 2, 1), AT @ y.T[:, :, None])\n",
        "\n",
        "    return sets.t(), solutions, None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoBerta"
      ],
      "metadata": {
        "id": "WVwPu5q0NJvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "STATE_PATH = '/content/drive/MyDrive/NLP-Final/roberta_state_w_finetuning'\n",
        "KEYS_VALUES_PATH = \"/content/drive/My Drive/NLP-Final/roberta_sparse_keys&values/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB_jQE8fmvZ-",
        "outputId": "63099b17-ef0c-4b4c-8048-f43560258f38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 34.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 31.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 24.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
            "Mounted at /content/drive\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1d_VSP0mhF4",
        "outputId": "f3095ad2-4fb6-4ff5-ad6c-e21c529747c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2000, 500, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/NLP-Final/'\n",
        "\n",
        "train_data = pd.read_csv(path + 'train.csv')\n",
        "test_data = pd.read_csv(path + 'test.csv')\n",
        "\n",
        "train_data = train_data[:2000]\n",
        "test_data = test_data[:500]\n",
        "\n",
        "train_data = train_data.to_dict(orient='records')\n",
        "test_data = test_data.to_dict(orient='records')\n",
        "type(train_data)\n",
        "\n",
        "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n",
        "\n",
        "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "def process_data(tokenizer):\n",
        "    train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
        "    test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n",
        "    train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "    test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "\n",
        "    train_y = np.array(train_labels) == 'pos'\n",
        "    test_y = np.array(test_labels) == 'pos'\n",
        "    # test_y = np.append(test_y, np.array(test_labels) != 'pos')\n",
        "\n",
        "    train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "    test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
        "    \n",
        "    train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "    train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "\n",
        "    test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "    test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "    train_masks_tensor = torch.tensor(train_masks)\n",
        "    test_masks_tensor = torch.tensor(test_masks)\n",
        "\n",
        "    train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "    test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "    test_sampler = SequentialSampler(test_dataset)\n",
        "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    return train_dataloader, test_dataloader, test_y"
      ],
      "metadata": {
        "id": "dDgKVk8amoHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "def train_model(model, train_dataloader):\n",
        "  param_optimizer = list(model.named_parameters()) \n",
        "  optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "  optimizer = Adam(model.parameters(), lr=3e-6)\n",
        "  torch.cuda.empty_cache()   # Clearing Cache space for a fresh Model run\n",
        "\n",
        "  for epoch_num in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "        print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
        "\n",
        "        tmp = torch.eq(labels, torch.zeros_like(labels))\n",
        "        labels_for_loss = torch.cat((labels, tmp), dim=1)\n",
        "        \n",
        "        logits = model(token_ids, masks).logits\n",
        "        \n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        batch_loss = loss_func(logits, labels_for_loss)\n",
        "        train_loss += batch_loss.item()\n",
        "        \n",
        "        model.zero_grad()\n",
        "        batch_loss.backward()\n",
        "\n",
        "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
        "  return model"
      ],
      "metadata": {
        "id": "tkLU74klE2bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, test_dataloader, test_y):\n",
        "  model_predicted = []\n",
        "  all_logits = []\n",
        "  with torch.no_grad():\n",
        "      for step_num, batch_data in enumerate(test_dataloader):\n",
        "          token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "          tmp = torch.eq(labels, torch.zeros_like(labels))\n",
        "          labels_for_loss = torch.cat((labels, tmp), dim=1)\n",
        "\n",
        "          logits = model(token_ids, masks).logits\n",
        "\n",
        "          loss_func = nn.CrossEntropyLoss()\n",
        "          loss = loss_func(logits, labels_for_loss)\n",
        "          numpy_logits = logits.cpu().detach().numpy()\n",
        "          \n",
        "          model_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "          all_logits += list(numpy_logits[:, 0])\n",
        "\n",
        "          token_ids, masks, labels = tuple(t.to(\"cpu\") for t in batch_data) # PATCH- move back to cpu\n",
        "  print(type(test_y)) \n",
        "  print(type(model_predicted))        \n",
        "  return classification_report(test_y, model_predicted, output_dict=True)"
      ],
      "metadata": {
        "id": "e7GeFbHWpcNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roberta without finetuning\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_dataloader, test_dataloader, test_y = process_data(roberta_tokenizer)\n",
        "roberta = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "report = eval_model(roberta, test_dataloader, test_y)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "h-x56xmS-pGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finetune Roberta\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_dataloader, test_dataloader, test_y = process_data(roberta_tokenizer)\n",
        "roberta = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "roberta = roberta.cuda()\n",
        "roberta_w_finetuning = train_model(roberta, train_dataloader)\n",
        "report = eval_model(roberta_w_finetuning, test_dataloader, test_y)\n",
        "print(report)\n",
        "\n",
        "STATE_PATH = '/content/drive/MyDrive/NLP-Final/roberta_state_w_finetuning'\n",
        "roberta_st = roberta_w_finetuning.state_dict()\n",
        "torch.save(roberta_st, STATE_PATH)"
      ],
      "metadata": {
        "id": "SvBRIb0nFSKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is how to load the model\n",
        "STATE_PATH = '/content/drive/MyDrive/NLP-Final/roberta_state_w_finetuning'\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_dataloader, test_dataloader, test_y = process_data(roberta_tokenizer)\n",
        "roberta_check = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "roberta_check = roberta_check.cuda()\n",
        "roberta_check.load_state_dict(torch.load(STATE_PATH))\n",
        "# eval_model(roberta_check, test_dataloader, test_y)"
      ],
      "metadata": {
        "id": "69L2EdDKdz_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_layers_params(model):\n",
        "  for name,x in model.named_parameters():\n",
        "    if \"intermediate.dense.weight\" in name: # intermidiate is the feed forward part of the layer \n",
        "      print(name)"
      ],
      "metadata": {
        "id": "oeLr36FXZEAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**EXPERIMENTS**"
      ],
      "metadata": {
        "id": "GAye2oIFCYKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Keys & Values Analysis"
      ],
      "metadata": {
        "id": "v4mYDRIj8teV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import savetxt\n",
        "LAYER_NUM = 11\n",
        "\n",
        "# Load fine-tuned GPT2\n",
        "roberta_analysis = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "roberta_analysis = roberta_analysis.cuda()\n",
        "roberta_analysis.load_state_dict(torch.load(STATE_PATH))\n",
        "\n",
        "# for name,x in gpt2_analysis.named_parameters():\n",
        "   # if \"mlp.c_fc.weight\" in name: # c_fc is the feed forward part of the layer \n",
        "      # print(name)\n",
        "\n",
        "# Get embeddings and BERT state\n",
        "embeddings = roberta_analysis.roberta.get_input_embeddings()\n",
        "E = embeddings.weight.detach().to(device)\n",
        "roberta_analysis_st = roberta_analysis.roberta.state_dict() # Bert model state dictionary\n",
        "\n",
        "# Compute sparse keys\n",
        "# keys = gpt2_analysis_st[f\"encoder.layer.{LAYER_NUM}.intermediate.dense.weight\"].detach().to(device)\n",
        "# n_non_zero_keys = max(int(0.1 * keys.shape[0]), 1)\n",
        "# sparse_keys = run_omp(E.T,  keys.T, n_non_zero_keys) # run optimized OMP\n",
        "# savetxt(KEYS_VALUES_PATH+f\"layer{LAYER_NUM}_keys.csv\", sparse_keys, delimiter=',')\n",
        "\n",
        "# Compute sparse values\n",
        "values = roberta_analysis_st[f\"encoder.layer.{LAYER_NUM}.output.dense.weight\"].detach().to(device)\n",
        "n_non_zero_values = max(int(0.1 * values.shape[1]), 1)\n",
        "sparse_values = run_omp(E.T,  values.T, n_non_zero_values) # run optimized OMP\n",
        "savetxt(KEYS_VALUES_PATH+f\"layer{LAYER_NUM}_values.csv\", sparse_values, delimiter=',')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "MOfbpCpM9Cm2",
        "outputId": "d0d35f87-e4c4-458b-b1db-c56d4a8a8e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-722eb499c845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta_analysis_st\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"encoder.layer.{LAYER_NUM}.output.dense.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mn_non_zero_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0msparse_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_omp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_non_zero_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run optimized OMP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKEYS_VALUES_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf\"layer{LAYER_NUM}_values.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-28a19464ad8f>\u001b[0m in \u001b[0;36mrun_omp\u001b[0;34m(X, y, n_nonzero_coefs, precompute, tol, normalize, fit_intercept, alg)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# If n_nonzero_coefs is equal to M, one should just return lstsq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'naive'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0msets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momp_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'v0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0msets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momp_v0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-28a19464ad8f>\u001b[0m in \u001b[0;36momp_naive\u001b[0;34m(X, y, n_nonzero_coefs, tol, XTX)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Trade b*k^2+bk+bkM = O(bkM) memory for much less compute time. (This has to be done anyways since we are batching,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# otherwise one could just permute columns of X in-place as in https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/linear_model/_omp.py#L28 )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mATs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mATys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mATAs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.70 GiB (GPU 0; 11.17 GiB total capacity; 10.05 GiB already allocated; 521.19 MiB free; 10.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import loadtxt\n",
        "LAYER_NUM = 11\n",
        "\n",
        "sparse_keys = loadtxt(KEYS_VALUES_PATH+f\"layer{LAYER_NUM}_keys.csv\", delimiter=',').astype(\"double\")\n",
        "sparse_keys = torch.from_numpy(sparse_keys).to(device)\n",
        "sparse_values = loadtxt(KEYS_VALUES_PATH+f\"layer{LAYER_NUM}_values.csv\", delimiter=',').astype(\"double\")\n",
        "sparse_values = torch.from_numpy(sparse_values).to(device)"
      ],
      "metadata": {
        "id": "5pU8_LMV826K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import loadtxt\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "sparse_keys = loadtxt(\"/content/drive/My Drive/NLP-Final/roberta_sparse_keys/results_omp_roberta_layer11.csv\", delimiter=',').astype(\"double\")\n",
        "sparse_keys = torch.from_numpy(sparse_keys).to(device)\n",
        "\n",
        "for i in range(100):\n",
        "  # non_zero_keys = torch.nonzero(sparse_keys[i])\n",
        "  sorted_keys, indices_keys = torch.sort(torch.abs(sparse_keys[i]), descending=True)\n",
        "  print(roberta_tokenizer.batch_decode(indices_keys[0:10]))\n",
        "\n",
        "  # non_zero_values = torch.nonzero(sparse_values[i])\n",
        "  # sorted_values, indices_values = torch.sort(torch.abs(sparse_values[i]), descending=True)\n",
        "  # print(roberta_tokenizer.batch_decode(indices_values[0:10]))\n",
        "  print(\"#################################################\")"
      ],
      "metadata": {
        "id": "STF4uo_69D8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c189f68-4a15-4e02-aeae-a77b7ad18244"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2', ' 12', ' session', ' Pl', ' real', 'ary', ' owned', ' students', ' selling', ' our']\n",
            "#################################################\n",
            "[' el', ' seasons', ' change', ' brain', ' simply', ' significant', '<s>', ' Jose', ' citizens', '17']\n",
            "#################################################\n",
            "[' struck', ' victim', 'm', ' made', ' kind', ' IN', ' really', ' survey', ' brain', 'ot']\n",
            "#################################################\n",
            "[' David', ' brother', ' though', ' De', ' make', ' treated', ' At', ' whole', ' voice', ' struck']\n",
            "#################################################\n",
            "[' figures', 'il', ' Star', ' men', 'So', '2018', ' UK', ' Black', ' movement', ' net']\n",
            "#################################################\n",
            "[' Cleveland', ' half', ' Florida', 'ty', ' baby', ' above', ' faces', 'ij', ' States', ' find']\n",
            "#################################################\n",
            "[' IN', ' inside', ' skin', ' waiting', ' looks', ' focus', ' international', 'q', 'x', ' den']\n",
            "#################################################\n",
            "[' IN', ' feeling', 'els', ' voice', ' Oct', ' subject', 'ale', ' into', ' 60', ' policy']\n",
            "#################################################\n",
            "[' Friday', ' have', ' boost', 'for', ' am', 'ay', ' abuse', ' den', ' tech', ' possible']\n",
            "#################################################\n",
            "[' kind', ' contract', ' mental', ' NBA', ' Black', ' Fire', ' growing', ' Br', ' mass', ' fan']\n",
            "#################################################\n",
            "[' London', ' Because', ' has', ' growth', 'ack', ' net', ' agreed', ' consumer', ' loan', '://']\n",
            "#################################################\n",
            "[' men', ' Because', '2018', 'ari', ' calling', ' IN', ' Nov', 'r', ' cross', ' Island']\n",
            "#################################################\n",
            "[' brain', ' skin', 'm', ' starts', ' States', ' C', ' double', 'ger', ' calling', ' focus']\n",
            "#################################################\n",
            "[' owned', ' previously', ' Fire', ' Fed', '�', ' why', ' C', ' application', ' connection', ' selling']\n",
            "#################################################\n",
            "[' larger', ' owned', 'So', ' really', ' better', ' activities', ' negative', ' den', ' cut', ' reports']\n",
            "#################################################\n",
            "['if', ' failed', ' Sen', ' Jack', ' above', ' signed', ' final', ' Syria', 'ations', ' Institute']\n",
            "#################################################\n",
            "[' Spain', ' Florida', ' men', ' London', 'By', \",'\", ' includes', ' played', ' consumer', ' among']\n",
            "#################################################\n",
            "[' debate', 'D', ' David', ' 12', ' tells', ' There', ' huge', ' statement', ' usually', ' facing']\n",
            "#################################################\n",
            "['yard', ' Korea', 'em', ' plant', ' Ed', ' read', ' projects', ' beyond', ' have', ' community']\n",
            "#################################################\n",
            "[' Cleveland', ' gold', 'yard', ' mission', ' calling', ' companies', ' Game', ' reasons', ' afternoon', ' solid']\n",
            "#################################################\n",
            "[' East', ' IN', ' jail', '.,', ' aren', ' his', ' leaving', ' am', 'q', '2']\n",
            "#################################################\n",
            "[' Korea', ' against', ' groups', ' Foundation', ' Good', ' like', ' guys', ' have', ' review', 'ke']\n",
            "#################################################\n",
            "[' will', ' which', ' storm', ' Because', 'pe', ' video', 'is', ' service', ' arrived', ' location']\n",
            "#################################################\n",
            "[\"'.\", 'ren', ' firms', ' Steve', ' am', ' tells', ' half', ' attorney', ' tournament', '6']\n",
            "#################################################\n",
            "[' IN', '45', ' however', ' food', ' trying', 'men', 'ai', ' Let', ' statement', ' sk']\n",
            "#################################################\n",
            "[' am', ' IN', ' artist', '<s>', ' addition', ' have', ' Korea', ' jail', 'yard', ' would']\n",
            "#################################################\n",
            "[' begin', ' Game', ' against', ' spread', ' form', 'The', ' consumer', ' multiple', ' Sen', 'ä']\n",
            "#################################################\n",
            "[' Korea', ' before', ' skin', 'By', ' husband', ' weight', ' storm', ' Show', ' early', 'ic']\n",
            "#################################################\n",
            "[' seven', ' am', ' truck', ' early', ' some', ' figures', 'im', ' Middle', ' storm', ' statements']\n",
            "#################################################\n",
            "[' London', ' le', ' consumer', 'ip', ' abuse', ' Clinton', 'so', 'il', ' green', ' among']\n",
            "#################################################\n",
            "[' blue', ' larger', ' mental', ' future', ' act', ' author', ' stop', ' begin', ' close', 'AT']\n",
            "#################################################\n",
            "[' Europe', 'line', 'els', ' mental', ' author', ' Prime', ' charges', 'field', 'ney', ' Pl']\n",
            "#################################################\n",
            "[' author', ' property', 'els', ' along', ' Turkey', ' old', ' London', ' District', ' statements', ' president']\n",
            "#################################################\n",
            "[' act', ' designed', ' sk', ' draft', ' second', 'ator', ' looking', ' Show', ' fresh', ' future']\n",
            "#################################################\n",
            "[' Because', ' April', 'yard', ' international', ' seven', ' think', ' artist', ' 24', ' rating', ' Am']\n",
            "#################################################\n",
            "[' Because', 'els', 'ite', ' bond', 'ad', 'izing', ' j', 'ED', ' Singapore', ' Friday']\n",
            "#################################################\n",
            "[' considering', ' career', ' tells', ' Club', 'ari', ' ability', ' hope', ' Wall', ' Korea', ' contract']\n",
            "#################################################\n",
            "[' begin', ' den', 'Reuters', ' Brazil', \" '\", '<s>', ' surprise', 'NYSE', ' talk', 'r']\n",
            "#################################################\n",
            "[' parents', '%.', ' Detroit', ' Ar', ' net', 'ien', ' London', ' coach', ' Of', ' difference']\n",
            "#################################################\n",
            "[' mental', ' 12', 'month', ' Spain', ' kind', ' began', ' seems', ' Ar', ' blue', ' den']\n",
            "#################################################\n",
            "[' struck', ' strength', ' Ar', ' car', ' higher', ' reasons', ' feels', ' paid', ' mission', ' Blue']\n",
            "#################################################\n",
            "['ed', ' pretty', ' Under', ' begin', ' Board', ' double', 'is', ' months', ' pressure', 'ica']\n",
            "#################################################\n",
            "['2', ' NBA', ' mark', ' really', 'age', ' offense', ' policy', '<s>', ' again', ' lines']\n",
            "#################################################\n",
            "[' IN', 'ations', 'By', ' selling', ' travel', ' poor', ' suggested', ' author', ' Grand', ' itself']\n",
            "#################################################\n",
            "['ack', ' voice', 'age', ' For', ' (', ' programme', ' solid', ' la', ' greater', ' doesn']\n",
            "#################################################\n",
            "[' NBA', ' act', ' author', 'ite', ' summer', ' train', ' al', 'iss', 'av', ' rating']\n",
            "#################################################\n",
            "[' Fire', ' vehicle', ' suggested', ' Images', 'm', ' researchers', ' East', ' Korea', ' investigation', ' General']\n",
            "#################################################\n",
            "[' j', ' States', 'ising', ' work', ' Federal', ' doubt', ' skin', ' top', ' recently', ' better']\n",
            "#################################################\n",
            "[' inside', ' almost', ' How', ' struck', ' scored', ' artist', '22', ' Financial', ' statement', ' themselves']\n",
            "#################################################\n",
            "[' larger', ' [', ' design', ' version', 'é', ' seven', ' wish', ' question', ' dividend', 'per']\n",
            "#################################################\n",
            "['for', ' taking', ' via', ' net', ' Texas', ' act', ' justice', 'res', ' car', ' tour']\n",
            "#################################################\n",
            "[' like', 'ite', ' suggested', ' brain', '2', 'ci', ' abuse', ' construction', ' sense', ' went']\n",
            "#################################################\n",
            "['age', ' order', ' fan', ' our', ' Turkey', ' international', ' property', ' Twitter', ' 31', ' include']\n",
            "#################################################\n",
            "[' commitment', ' session', 'age', ' Minister', ' tech', 'for', ' runs', ' story', ' Best', ' brain']\n",
            "#################################################\n",
            "[' bond', 'IN', ' tells', ' solid', ' Minister', ' statement', ' international', ' like', ' Y', 'NYSE']\n",
            "#################################################\n",
            "['By', ' Because', ' brand', '�', ' month', ' spread', ' overall', ' met', ' medical', ' catch']\n",
            "#################################################\n",
            "[' NBA', ' Ed', ' international', ' programme', 'r', ' victim', ' find', ' Br', ' struck', ' Team']\n",
            "#################################################\n",
            "[' decision', ' player', ' really', 'ed', ' limited', ' offense', ' waiting', ' hold', ' deal', ' house']\n",
            "#################################################\n",
            "[' larger', 'yard', ' would', ' storm', 'age', 'er', ' offering', ' affected', ' Nov', ' unique']\n",
            "#################################################\n",
            "[' Bill', ' Because', ' reserved', ' setting', ' community', ' does', 'E', 'ale', 'ut', 'month']\n",
            "#################################################\n",
            "[' travel', ' being', ' benefit', 'age', ' upon', ' making', ' faces', ' who', ' players', ' resources']\n",
            "#################################################\n",
            "[' baby', ' Fire', ' space', ' excited', ' Florida', ' months', ' led', 'yard', ' victim', ' East']\n",
            "#################################################\n",
            "[' 2010', ' straight', ' form', 'ney', ' Angeles', ' statement', '99', ' significant', ' picture', ' game']\n",
            "#################################################\n",
            "[' Because', ' really', ' bond', ' room', ' broadcast', ' overall', ' have', ' rates', ' red', ' nuclear']\n",
            "#################################################\n",
            "[' East', ' mean', ' signs', ' Ministry', ' order', ' week', '32', 'm', ' 2016', ' storm']\n",
            "#################################################\n",
            "[' Team', ' act', ' Minister', ' all', ' contract', ' 2015', 'A', ' heart', ' sure', ' age']\n",
            "#################################################\n",
            "[' Australian', ' operations', ' other', ' will', ' economy', ' 15', ' waiting', ' se', ' cities', ' While']\n",
            "#################################################\n",
            "['if', ' cause', ' developed', ' trust', ' gas', ' solid', ' save', ' nine', ' capital', ' nation']\n",
            "#################################################\n",
            "['els', 'ack', ' tells', 'By', ' information', ' usually', ' question', ' Judge', 'yard', ' professional']\n",
            "#################################################\n",
            "['yard', ' reached', ' brother', ' books', ' 12', ' played', ' perform', ' usually', ' am', ' Advertisement']\n",
            "#################################################\n",
            "[' act', ' seven', ' skin', ' Good', 'ats', ' states', ' series', ' big', ' students', ' Cor']\n",
            "#################################################\n",
            "[' De', 'NYSE', ' 3', ' control', 're', ' Red', ' David', ' Nov', ' setting', '13']\n",
            "#################################################\n",
            "[' really', 'els', ' setting', ' second', ' offering', 'By', ' team', ' s', ' has', ' Korea']\n",
            "#################################################\n",
            "[' Start', 'as', ' much', ' 2011', '2', ' upon', ' den', 'ari', 'ful', ' forced']\n",
            "#################################################\n",
            "[' all', ' meetings', ' girl', ' 8', ' Because', ' victim', ' digital', 'if', ' truck', ' like']\n",
            "#################################################\n",
            "[' IN', ' am', ' Board', ' Fire', ' surprise', 'TS', ' travel', '_', 'ai', ' estate']\n",
            "#################################################\n",
            "[' trading', ' victim', ' down', ' mental', 'ger', 'ari', ' artist', ' margin', ' anyone', ' Copyright']\n",
            "#################################################\n",
            "['the', ' bid', ' really', ' Chicago', ' space', ' managed', ' hair', ' When', ' months', ' this']\n",
            "#################################################\n",
            "[' overall', ' really', 'yard', 'the', 'ica', 'Reporting', ' reasons', 'il', 'By', ' include']\n",
            "#################################################\n",
            "[' rating', ' waiting', ' Because', ' author', ' David', ' trust', ' vehicle', ' history', ' cent', ' speed']\n",
            "#################################################\n",
            "[' IN', ' again', 'TS', ' failed', 'us', ' na', ' Oct', 'month', ' like', ' almost']\n",
            "#################################################\n",
            "[' FBI', 'ig', ' completed', ' latest', ' career', ' state', ' connection', 'ng', ' parents', ' patients']\n",
            "#################################################\n",
            "['the', ' eye', ' both', ' account', ' Black', ' 2018', '://', ' driver', ' Fire', ' conduct']\n",
            "#################################################\n",
            "[' Law', ' difference', ' his', ' le', ' if', ' firm', ' review', ' all', ' East', ' August']\n",
            "#################################################\n",
            "[' author', ' statement', ' trust', ' Clinton', ' States', ' another', ' changes', ' 2007', ' gold', ' sanctions']\n",
            "#################################################\n",
            "[' almost', ' French', 'So', ' Click', ' Sen', ' really', 'q', ' excited', ' London', 'ans']\n",
            "#################################################\n",
            "[' usually', ' age', ' healthy', ' ratio', ' hitting', ' contract', ' Andrew', ' immigration', ' victim', ' Alabama']\n",
            "#################################################\n",
            "[' Clinton', ' like', ' watch', ' simply', ' point', 'ka', ' 30', ' coming', 'ah', ' innings']\n",
            "#################################################\n",
            "[' programme', 'age', ' calls', ' center', ' history', ' Supreme', ' Rob', ' begin', ' youth', 'ary']\n",
            "#################################################\n",
            "[' facing', ' material', 'itt', ' against', 'q', 'TS', ' Sunday', ' girl', ' Sen', ' husband']\n",
            "#################################################\n",
            "[' net', ' sanctions', 'IC', ' holiday', ' early', ' debate', 'ra', ' Spain', ' IN', ' ones']\n",
            "#################################################\n",
            "['age', ' contract', 'ub', ' group', ' session', 'm', ' Mc', '24', 'NYSE', 'org']\n",
            "#################################################\n",
            "['.,', ' J', ' room', ' strong', ' mean', '://', ' red', ' books', ' immigration', ' vs']\n",
            "#################################################\n",
            "[\",'\", ' title', ' half', ' larger', ' De', '!', ' solution', ' 26', ' personal', ' zone']\n",
            "#################################################\n",
            "['By', ' Korea', ' Pl', 'r', ' international', ' abuse', 'ere', ' positions', ' made', ' Y']\n",
            "#################################################\n",
            "[' along', ' group', ' making', 'ari', ' profit', ' enjoy', ' buying', ' act', ' job', ' airport']\n",
            "#################################################\n",
            "[' international', ' explained', ' failed', ' Bowl', ' brain', ' sk', ' researchers', ' crowd', ' States', ' consumer']\n",
            "#################################################\n",
            "[' De', ' half', ' point', 'the', ' why', '://', ' really', ' check', ' mission', ' driving']\n",
            "#################################################\n",
            "[' like', ' 30', ' bond', ' Fire', ' Sen', '19', ' almost', ' future', ' chance', 'em']\n",
            "#################################################\n",
            "[' Oct', ' losing', ' international', ' picked', '.\"', ' poor', ' determined', ' 60', 'ari', ' Prime']\n",
            "#################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yuvi - Fast OMP"
      ],
      "metadata": {
        "id": "xh-lDOF-tvNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STATE_PATH = '/content/drive/MyDrive/NLP-Final/roberta_state_w_finetuning'\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_dataloader, test_dataloader, test_y = process_data(roberta_tokenizer)\n",
        "roberta_fast_omp = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "roberta_fast_omp = roberta_fast_omp.cuda()\n",
        "roberta_fast_omp.load_state_dict(torch.load(STATE_PATH))\n",
        "roberta_fast_omp.eval()"
      ],
      "metadata": {
        "id": "9P5BN8xGt6EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb1 = roberta_fast_omp.roberta.get_input_embeddings() # embeddings\n",
        "X_tensor = emb1.weight.detach().to(device) \n",
        "num_features = X_tensor.shape[1]\n",
        "n_non_zero = max(int(0.1 * num_features), 1) # 10% of X num of features\n",
        "\n",
        "roberta_fast_omp_st = roberta_fast_omp.roberta.state_dict() # model state dictionary\n",
        "\n",
        "torch.cuda.empty_cache() # Clear memory\n",
        "\n",
        "y_tensor = roberta_fast_omp_st[f\"encoder.layer.0.intermediate.dense.weight\"].detach().to(device) # get weights for 0 ff layer\n",
        "\n",
        "xes = run_omp(X_tensor.T,  y_tensor, n_non_zero) # run fast OMP\n",
        "temp_weights = xes @ X_tensor # matrix mult to get the weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bqtR9E6uLQC",
        "outputId": "2eef482b-89bf-412d-89e5-66b6ba8fe106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
            "L = torch.cholesky(A)\n",
            "should be replaced with\n",
            "L = torch.linalg.cholesky(A)\n",
            "and\n",
            "U = torch.cholesky(A, upper=True)\n",
            "should be replaced with\n",
            "U = torch.linalg.cholesky(A).transpose(-2, -1).conj().\n",
            "This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1285.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import savetxt, loadtxt\n",
        "SPARSE_KEYS_DIR = '/content/drive/MyDrive/NLP-Final/roberta_sparse_keys/'\n",
        "STATE_PATH = '/content/drive/MyDrive/NLP-Final/roberta_state_w_finetuning'\n",
        "\n",
        "xes2 = loadtxt(SPARSE_KEYS_DIR+\"results_omp_roberta_layer0.csv\", delimiter=',').astype(\"double\")\n",
        "xes2 = torch.from_numpy(xes2).to(device)\n",
        "\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_dataloader, test_dataloader, test_y = process_data(roberta_tokenizer)\n",
        "\n",
        "# global roberta\n",
        "roberta_global = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "roberta_global.load_state_dict(torch.load(STATE_PATH))\n",
        "\n",
        "emb1 = roberta_global.roberta.get_input_embeddings() # embeddings\n",
        "X_tensor = emb1.weight.detach().to(device) \n",
        "num_features = X_tensor.shape[1]\n",
        "n_non_zero = max(int(0.1 * num_features), 1) # 10% of X num of features\n",
        "\n",
        "temp_weights = xes2.T @ X_tensor.T # matrix mult to get the weights\n",
        "\n",
        "roberta_global_st = roberta_global.roberta.state_dict() # model state dictionary\n",
        "y_tensor = roberta_global_st[f\"encoder.layer.0.intermediate.dense.weight\"].detach().to(device) # get weights for ith ff layer\n",
        "\n",
        "roberta_global_st[f\"encoder.layer.{i}.intermediate.dense.weight\"]  = temp_weights\n",
        "roberta_global.roberta.load_state_dict(roberta_global_st)\n",
        "\n",
        "report = eval_model(roberta_global, test_dataloader, test_y)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "_l3xlLIivUgJ",
        "outputId": "36e4ba5b-1bb1-4353-f0d7-c53aa8a37234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3ef32fc26ff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mn_non_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 10% of X num of features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtemp_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxes2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m# matrix mult to get the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mroberta_global_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta_global\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Bert model state dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yuvi SKlearn Test"
      ],
      "metadata": {
        "id": "vxm8QiFjiWRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Roberta after finetuning\n",
        "STATE_PATH = '/content/drive/MyDrive/NLP-Final/roberta_state_w_finetuning'\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_dataloader, test_dataloader, test_y = process_data(roberta_tokenizer)\n",
        "roberta1 = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "roberta1 = roberta1.cuda()\n",
        "roberta1.load_state_dict(torch.load(STATE_PATH))\n",
        "# eval_model(roberta1, test_dataloader, test_y)\n",
        "# roberta1.load_state_dict(torch.load(STATE_PATH, map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "4300a7567e6044ef99cede8f69295912",
            "50355d4373c04aae8dea0eb07ae539d0",
            "70baae3535c24251b062e8c240754a6c",
            "0280d7fa237441669cd9b09ed14ba814",
            "0b99669061e942628a4c043cc2cfe3fe",
            "b1f72527adb649f6b676ca1e6a214ad5",
            "5810a90db0bf4fd4a389a73056e0c0e9",
            "f246208aafd74c5aa6c49b3e2d353855",
            "a7fc7e446415436e897913e74045751d",
            "6dfafafd2b2e495eaf425e2349b66af2",
            "3da428bccf624a838fa9f3988989eb3f",
            "7961a2057ff74d449791dd8e5b0fc5c6",
            "8f4e6cd8af194206b2e3f0d3296dd605",
            "ea6a488b03d44ddabb3092dfcac67e64",
            "85f515666b00486ca51ac079d85d9ad1",
            "48dea6c14d2e448bacf264d5b31b2051",
            "5fc46f06615d42e1bc18e37140915945",
            "3cf091c799fd4d7fad810268c90093b8",
            "13ba322aff2d4dafb08fe951b0924c43",
            "e3de4d6a57984bb287bbfce14999948f",
            "886560bfcda54e9097b0b1c2156c21bb",
            "216a3dd09a574209b2c4cb715f98b783",
            "e5a102641fb04d318ff015259a30b82a",
            "24ea5d63a37d47af8b0348f9b5053b3a",
            "d433400f596c49aea23dbcd806c251c7",
            "b3f000f3346f4796bcbe30c2672179d3",
            "25709262d4394cd08da243d6e22b2d34",
            "90546a14afee4c62b58480c60b38df80",
            "56b66e6b9b9345d7a827267175f33df1",
            "9511d4f1fd8441b3a2d36e3b0db1711c",
            "71bf2722a7b345368b65ed465bdc83af",
            "73b1642c4417446dbe867e1f609c61b3",
            "57e026e067d44905b0d6e4fd63f035f1",
            "507d1884e9fe464c81efaf50516d0b73",
            "5a87a665b9454da4b2e862367c88a53e",
            "bee2a6aa1aa0479197c40e3b14db61cd",
            "506c6deafff44a3da063651929e28422",
            "b816d7ec868842e5a6aafdb1e15607a6",
            "479d1b31d5c34fc68c1c98fdfe6ae484",
            "a06de19af9f74c259c96ab63464bd95e",
            "dc8c402a746a43829aded077aeb1ab99",
            "d5f8881ae8da41cc97ce46422c556e1f",
            "b653a34c462e44a8af8846d62687007e",
            "3d0492b16e204b8da0e91b345bff3921"
          ]
        },
        "id": "MWYboQNSSfOl",
        "outputId": "42133731-dc04-487a-e91e-dff7b37d83a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4300a7567e6044ef99cede8f69295912"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7961a2057ff74d449791dd8e5b0fc5c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5a102641fb04d318ff015259a30b82a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "507d1884e9fe464c81efaf50516d0b73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "\n",
        "emb1 = roberta1.roberta.get_input_embeddings() # embeddings\n",
        "X = emb1.weight.detach()\n",
        "num_features = X.shape[1]\n",
        "n_non_zero = max(int(0.1 * num_features), 1) # 10% of X num of features\n",
        "\n",
        "roberta1_st = roberta1.roberta.state_dict() # model state dictionary\n",
        "\n",
        "torch.cuda.empty_cache() # Clear memory\n",
        "\n",
        "y = roberta1_st[f\"encoder.layer.0.intermediate.dense.weight\"].detach() # get weights for 0 ff layer\n",
        "reg = OrthogonalMatchingPursuit(normalize=False).fit(X.T.cpu(),  y.T.cpu())\n",
        "xes = reg.predict(X.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg8UaY2pYwqu",
        "outputId": "66e36405-2813-4773-f362-d9c6bbaffb70"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
            "  X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import savetxt, loadtxt\n",
        "savetxt(f\"/content/drive/MyDrive/NLP-Final/results_omp_roberta_layer0.csv\", xes, delimiter=',')"
      ],
      "metadata": {
        "id": "xXcmt58JcCDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replace every state untouching the others"
      ],
      "metadata": {
        "id": "9O8qdJNrCfyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "\n",
        "# Replace every state untouching the others\n",
        "emb1 = roberta1.roberta.get_input_embeddings() # embeddings\n",
        "roberta1_st = roberta1.roberta.state_dict() # Bert model state dictionary\n",
        "\n",
        "X_tensor = emb1.weight.detach().to(device) \n",
        "num_features = X_tensor.shape[1]\n",
        "n_non_zero = max(int(0.1 * num_features), 1) # 10% of X num of features\n",
        "\n",
        "exp_results = dict()\n",
        "\n",
        "for i in range(13):\n",
        "  torch.cuda.empty_cache() # Clear memory\n",
        "  roberta1.load_state_dict(torch.load(STATE_PATH, map_location=torch.device('cpu'))) # load first the original weights\n",
        "\n",
        "  if i == 0:\n",
        "    # print(\"Results Before Replacement\")\n",
        "    # res = eval_model(roberta1, test_dataloader, test_y)\n",
        "    # exp_results[f'No_Replace'] = res\n",
        "    continue\n",
        "\n",
        "  roberta1_st = roberta1.roberta.state_dict() # Bert model state dictionary\n",
        "  y_tensor = roberta1_st[f\"encoder.layer.{i-1}.intermediate.dense.weight\"].detach().to(device) # get weights for ith ff layer\n",
        "\n",
        "  # xes = run_omp(X_tensor.T,  y_tensor, n_non_zero) # run fast OMP\n",
        "  reg = OrthogonalMatchingPursuit(normalize=False).fit(X_tensor.T,  y_tensor)\n",
        "  xes = reg.predict(X_tensor.T)\n",
        "\n",
        "  temp_weights = xes @ X_tensor # matrix mult to get the weights\n",
        "\n",
        "  roberta1_st[f\"encoder.layer.{i-1}.intermediate.dense.weight\"]  = temp_weights\n",
        "  roberta1.bert.load_state_dict(roberta1_st)\n",
        "  print(f\"Results for layer-{i-1} Weights Replacement\")\n",
        "  res = eval_model(roberta1)\n",
        "  exp_results[f'layer-{i-1}'] = res\n",
        "\n",
        "\n",
        "  y_tensor.to(\"cpu\")\n",
        "\n",
        "pd.DataFrame.from_dict(exp_results).to_excel(f'/content/drive/MyDrive/NLP-Final/experiments_results/Roberta/Replace_each_layer_leaving_others_untouched.xlsx')"
      ],
      "metadata": {
        "id": "S75RODMNCWkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replace Every layer one after the other"
      ],
      "metadata": {
        "id": "30ISrJCICoWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add replace and test -  in every step \n",
        "emb1 = bert1.bert.get_input_embeddings() # embeddings\n",
        "bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "\n",
        "X_tensor = emb1.weight.detach().to(device) \n",
        "num_features = X_tensor.shape[1]\n",
        "n_non_zero = max(int(0.1 * num_features), 1) # 10% of X num of features\n",
        "\n",
        "exp_results = dict()\n",
        "\n",
        "for i in range(13):\n",
        "  torch.cuda.empty_cache() # Clear memory\n",
        "  bert1.load_state_dict(torch.load(STATE_PATH)) # load first the original weights\n",
        "\n",
        "  if i == 0:\n",
        "    print(\"Results Before Replacement\")\n",
        "    res = eval_model(bert1)\n",
        "    exp_results[f'No_Replace'] = res\n",
        "    continue\n",
        "\n",
        "  # bert1_st is updated every step\n",
        "  y_tensor = bert1_st[f\"encoder.layer.{i-1}.intermediate.dense.weight\"].detach().to(device) # get weights for ith ff layer\n",
        "\n",
        "  xes = run_omp(X_tensor.T,  y_tensor, n_non_zero) # run fast OMP\n",
        "\n",
        "  temp_weights = xes @ X_tensor # matrix mult to get the weights\n",
        "\n",
        "  bert1_st[f\"encoder.layer.{i-1}.intermediate.dense.weight\"]  = temp_weights\n",
        "  bert1.bert.load_state_dict(bert1_st)\n",
        "  print(f\"Results for Replacing all the layers up to layer-{i-1} (included)\")\n",
        "  res = eval_model(bert1)\n",
        "  exp_results[f'layer0_to_{i}'] = res\n",
        "\n",
        "pd.DataFrame.from_dict(exp_results).to_excel(f'/content/drive/MyDrive/NLP-Final/experiments_results/Roberta/Replace_layers_one_by_one.xlsx')"
      ],
      "metadata": {
        "id": "dYGNOVwQEZfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparsity Levels Experiment - 1% - 45% - for each layer"
      ],
      "metadata": {
        "id": "yh1303Z8GO8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1%-45%\n",
        "import gc \n",
        "emb1 = bert1.bert.get_input_embeddings() # embeddings\n",
        "bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "\n",
        "X_tensor = emb1.weight.detach().to(device) \n",
        "num_features = X_tensor.shape[1]\n",
        "\n",
        "\n",
        "ratios = [0.0,0.01,0.05,0.1,0.15,0.2,0.3,0.35,0.4,0.45]\n",
        "\n",
        "for j in range (12):\n",
        "  gc.collect()\n",
        "  exp_results = dict()\n",
        "  for i, ratio in enumerate(ratios):\n",
        "    # Change ratio of non_zero_coef each iteration\n",
        "    n_non_zero = max(int(ratio * num_features), 1) # ratio of X num of features\n",
        "    torch.cuda.empty_cache() # Clear memory\n",
        "    \n",
        "    bert1.load_state_dict(torch.load(STATE_PATH)) # load first the original weights\n",
        "\n",
        "    if i == 0: # print baseline results first\n",
        "      print(\"Results Before Replacement\")\n",
        "      eval_model(bert1)\n",
        "      continue\n",
        "\n",
        "    bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "    y_tensor = bert1_st[f\"encoder.layer.{j}.intermediate.dense.weight\"].detach().to(device) # get weights for ith ff layer\n",
        "\n",
        "    xes = run_omp(X_tensor.T,  y_tensor, n_non_zero) # run fast OMP\n",
        "\n",
        "    temp_weights = xes @ X_tensor # matrix mult to get the weights\n",
        "\n",
        "    bert1_st[f\"encoder.layer.{j}.intermediate.dense.weight\"]  = temp_weights\n",
        "    bert1.bert.load_state_dict(bert1_st)\n",
        "    print(f\"Results for layer-{j} Weights Replacement with {100*ratio:.2f}% Non-Zero Coefficients\")\n",
        "    res = eval_model(bert1)\n",
        "    exp_results[f'{100*ratio:.2f}%'] = res\n",
        "    y_tensor.to(\"cpu\")\n",
        "\n",
        "  pd.DataFrame.from_dict(exp_results).to_excel(f'/content/drive/MyDrive/NLP-Final/experiments_results/Roberta/Sparsity_Levels_1_45_layer{j}.xlsx')\n"
      ],
      "metadata": {
        "id": "iL7JZuLZGbqU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}