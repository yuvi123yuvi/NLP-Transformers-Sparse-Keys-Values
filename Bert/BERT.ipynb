{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXhsEvzIjlZP"
      },
      "source": [
        "#Fast OMP Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VM6DBfeWjtT6"
      },
      "outputs": [],
      "source": [
        "%load_ext Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_wQlHgEOj2lW"
      },
      "outputs": [],
      "source": [
        "%%cython\n",
        "\n",
        "\"\"\"Cython allows us to call lower level c-code instead of using Python. It can be a surprisingly big speedup!\"\"\"\n",
        "import cython\n",
        "from scipy.linalg.cython_blas cimport idamax, isamax, daxpy, dgemv, dtrmv, dcopy\n",
        "from scipy.linalg.cython_lapack cimport dposv, dppsv, sppsv\n",
        "\n",
        "ctypedef fused proj_t:\n",
        "    double\n",
        "    float\n",
        "\n",
        "@cython.boundscheck(False)\n",
        "@cython.wraparound(False)\n",
        "cpdef void ppsv(proj_t[:, :] As,\n",
        "           proj_t[:, :, :] ys) nogil:\n",
        "    # Works not for strided array I think. And please do not give a negative-stride array.\n",
        "    cdef Py_ssize_t B = ys.shape[0]  # Batch size\n",
        "    cdef int N = ys.shape[1]\n",
        "    cdef int nrhs = ys.shape[2]\n",
        "    cdef int info = 0  # Just discard any error signals ;)\n",
        "    cdef char uplo = 85 # The letter 'U', since we store the lower triangle and fortran sees As.T.\n",
        "    # cdef int ldb = ys[0].strides[0] // sizeof(double)\n",
        "\n",
        "    for i from 0 <= i < B:\n",
        "        if proj_t is double:  # One C-function is created for each of these specializations! :) (see argmax_blast.__signatures__)\n",
        "            dppsv(&uplo, &N, &nrhs, &As[i, 0], &ys[i, 0, 0], &N, &info)\n",
        "        elif proj_t is float:\n",
        "            sppsv(&uplo, &N, &nrhs, &As[i, 0], &ys[i, 0, 0], &N, &info)\n",
        "\n",
        "\n",
        "@cython.boundscheck(False)\n",
        "@cython.wraparound(False)\n",
        "cpdef void argmax_blast(proj_t[:, :] projections,\n",
        "                 long long[:] output) nogil:\n",
        "    # TODO: Numpy has its own indexing data-type - this may be a more appropriate output, and may even be faster.\n",
        "    # http://conference.scipy.org/static/wiki/seljebotn_cython.pdf\n",
        "    # https://apprize.best/python/cython/3.html\n",
        "    cdef Py_ssize_t B = projections.shape[0]\n",
        "    cdef int N = projections.shape[1]\n",
        "    cdef int incx = projections.strides[1] // sizeof(proj_t)  # Stride between elements.\n",
        "    cdef Py_ssize_t i\n",
        "    for i from 0 <= i < B:\n",
        "        if proj_t is double:\n",
        "            output[i] = idamax(&N, &projections[i, 0], &incx) - 1\n",
        "        elif proj_t is float:\n",
        "            output[i] = isamax(&N, &projections[i, 0], &incx) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XYR0qH2Xj73q"
      },
      "outputs": [],
      "source": [
        "\"\"\"This cell contains the code we've implemented. You should be able to call each function directly, or alternatively, see our example calls below\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.utils\n",
        "import torch.utils.data\n",
        "from sklearn.datasets import make_sparse_coded_signal\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "from contextlib import contextmanager\n",
        "from timeit import default_timer\n",
        "# from test_omp import omp_naive\n",
        "# from test import *  # FIXME: better name\n",
        "# from line_profiler import line_profiler\n",
        "\n",
        "n_components, n_features = 100, 100\n",
        "n_nonzero_coefs = 17\n",
        "n_samples = 50\n",
        "\n",
        "@contextmanager\n",
        "def elapsed_timer():\n",
        "    # https://stackoverflow.com/questions/7370801/how-to-measure-elapsed-time-in-python\n",
        "    start = default_timer()\n",
        "    elapser = lambda: default_timer() - start\n",
        "    yield lambda: elapser()\n",
        "    end = default_timer()\n",
        "    elapser = lambda: end-start\n",
        "\n",
        "\n",
        "def run_omp(X, y, n_nonzero_coefs, precompute=True, tol=0.0, normalize=False, fit_intercept=False, alg='naive'):\n",
        "    if not isinstance(X, torch.Tensor):\n",
        "        X = torch.as_tensor(X)\n",
        "        y = torch.as_tensor(y)\n",
        "\n",
        "    # We can either return sets, (sets, solutions), or xests\n",
        "    # These are all equivalent, but are simply more and more dense representations.\n",
        "    # Given sets and X and y one can (re-)construct xests. The second is just a sparse vector repr.\n",
        "\n",
        "    # https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/linear_model/_omp.py#L690\n",
        "    if fit_intercept or normalize:\n",
        "        X = X.clone()\n",
        "        assert not isinstance(precompute, torch.Tensor), \"If user pre-computes XTX they can also pre-normalize X\" \\\n",
        "                                                         \" as well, so normalize and fit_intercept must be set false.\"\n",
        "\n",
        "    if fit_intercept:\n",
        "        X = X - X.mean(0)\n",
        "        y = y - y.mean(1)[:, None]\n",
        "\n",
        "    # To keep a good condition number on X, especially with Cholesky compared to LU factorization,\n",
        "    # we should probably always normalize it (OMP is invariant anyways)\n",
        "    if normalize is True:  # User can also just optionally supply pre-computed norms.\n",
        "        normalize = (X * X).sum(0).sqrt()\n",
        "        X /= normalize[None, :]\n",
        "\n",
        "    if precompute is True or alg == 'v0':\n",
        "        precompute = X.T @ X\n",
        "\n",
        "    # If n_nonzero_coefs is equal to M, one should just return lstsq\n",
        "    if alg == 'naive':\n",
        "        sets, solutions, lengths = omp_naive(X, y, n_nonzero_coefs=n_nonzero_coefs, XTX=precompute, tol=tol)\n",
        "    elif alg == 'v0':\n",
        "        sets, solutions, lengths = omp_v0(X, y, n_nonzero_coefs=n_nonzero_coefs, XTX=precompute, tol=tol)\n",
        "\n",
        "\n",
        "    solutions = solutions.squeeze(-1)\n",
        "    if normalize is not False:\n",
        "        solutions /= normalize[sets]\n",
        "\n",
        "    xests = y.new_zeros(y.shape[0], X.shape[1])\n",
        "    if lengths is None:\n",
        "        xests[torch.arange(y.shape[0], dtype=sets.dtype, device=sets.device)[:, None], sets] = solutions\n",
        "    else:\n",
        "        for i in range(y.shape[0]):\n",
        "            # xests[i].scatter_(-1, sets[i, :lengths[i]], solutions[i, :lengths[i]])\n",
        "            xests[i, sets[i, :lengths[i]]] = solutions[i, :lengths[i]]\n",
        "\n",
        "    return xests\n",
        "\n",
        "def batch_mm(matrix, matrix_batch, return_contiguous=True):\n",
        "    \"\"\"\n",
        "    :param matrix: Sparse or dense matrix, size (m, n).\n",
        "    :param matrix_batch: Batched dense matrices, size (b, n, k).\n",
        "    :return: The batched matrix-matrix product, size (m, n) x (b, n, k) = (b, m, k).\n",
        "    \"\"\"\n",
        "    # One dgemm is faster than many dgemv.\n",
        "    # From https://github.com/pytorch/pytorch/issues/14489#issuecomment-607730242\n",
        "    batch_size = matrix_batch.shape[0]\n",
        "    # Stack the vector batch into columns. (b, n, k) -> (n, b, k) -> (n, b*k)\n",
        "    vectors = matrix_batch.transpose([1, 0, 2]).reshape(matrix.shape[1], -1)\n",
        "\n",
        "    # A matrix-matrix product is a batched matrix-vector product of the columns.\n",
        "    # And then reverse the reshaping. (m, n) x (n, b*k) = (m, b*k) -> (m, b, k) -> (b, m, k)\n",
        "    if return_contiguous:\n",
        "        result = np.empty_like(matrix_batch, shape=(batch_size, matrix.shape[0], matrix_batch.shape[2]))\n",
        "        np.matmul(matrix, vectors, out=result.transpose([1, 0, 2]).reshape(matrix.shape[0], -1))\n",
        "    else:\n",
        "        result = (matrix @ vectors).reshape(matrix.shape[0], batch_size, -1).transpose([1, 0, 2])\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def innerp(x, y=None, out=None):\n",
        "    if y is None:\n",
        "        y = x\n",
        "    if out is not None:\n",
        "        out = out[:, None, None]  # Add space for two singleton dimensions.\n",
        "    return torch.matmul(x[..., None, :], y[..., :, None], out=out)[..., 0, 0]\n",
        "\n",
        "def cholesky_solve(ATA, ATy):\n",
        "    if ATA.dtype == torch.half or ATy.dtype == torch.half:\n",
        "        return ATy.to(torch.float).cholesky_solve(torch.cholesky(ATA.to(torch.float))).to(ATy.dtype)\n",
        "    return ATy.cholesky_solve(torch.cholesky(ATA)).to(ATy.dtype)\n",
        "\n",
        "\n",
        "def omp_naive(X, y, n_nonzero_coefs, tol=None, XTX=None):\n",
        "    on_cpu = not (y.is_cuda or y.dtype == torch.half)\n",
        "    # torch.cuda.synchronize()\n",
        "    # Given X as an MxN array and y as an BxN array, do omp to approximately solve Xb=y\n",
        "\n",
        "    # Base variables\n",
        "    XT = X.contiguous().t()  # Store XT in fortran-order.\n",
        "    y = y.contiguous()\n",
        "    r = y.clone()\n",
        "\n",
        "    sets = y.new_zeros((n_nonzero_coefs, y.shape[0]), dtype=torch.long).t()\n",
        "    if tol:\n",
        "        result_sets = sets.new_zeros(y.shape[0], n_nonzero_coefs)\n",
        "        result_lengths = sets.new_zeros(y.shape[0])\n",
        "        result_solutions = y.new_zeros((y.shape[0], n_nonzero_coefs, 1))\n",
        "        original_indices = torch.arange(y.shape[0], dtype=sets.dtype, device=sets.device)\n",
        "\n",
        "    # Trade b*k^2+bk+bkM = O(bkM) memory for much less compute time. (This has to be done anyways since we are batching,\n",
        "    # otherwise one could just permute columns of X in-place as in https://github.com/scikit-learn/scikit-learn/blob/15a949460dbf19e5e196b8ef48f9712b72a3b3c3/sklearn/linear_model/_omp.py#L28 )\n",
        "    ATs = y.new_zeros(r.shape[0], n_nonzero_coefs, X.shape[0])\n",
        "    ATys = y.new_zeros(r.shape[0], n_nonzero_coefs, 1)\n",
        "    ATAs = torch.eye(n_nonzero_coefs, dtype=y.dtype, device=y.device)[None].repeat(r.shape[0], 1, 1)\n",
        "    if on_cpu:\n",
        "        # For CPU it is faster to use a packed representation of the lower triangle in ATA.\n",
        "        tri_idx = torch.tril_indices(n_nonzero_coefs, n_nonzero_coefs, device=sets.device, dtype=sets.dtype)\n",
        "        ATAs = ATAs[:, tri_idx[0], tri_idx[1]]\n",
        "\n",
        "    solutions = y.new_zeros((r.shape[0], 0))\n",
        "\n",
        "    for k in range(n_nonzero_coefs+bool(tol)):\n",
        "        # STOPPING CRITERIA\n",
        "        if tol:\n",
        "            problems_done = innerp(r) <= tol\n",
        "            if k == n_nonzero_coefs:\n",
        "                problems_done[:] = True\n",
        "\n",
        "            if problems_done.any():\n",
        "                remaining = ~problems_done\n",
        "\n",
        "                orig_idxs = original_indices[problems_done]\n",
        "                result_sets[orig_idxs, :k] = sets[problems_done, :k]\n",
        "                result_solutions[orig_idxs, :k] = solutions[problems_done]\n",
        "                result_lengths[orig_idxs] = k\n",
        "                original_indices = original_indices[remaining]\n",
        "\n",
        "                # original_indices = original_indices[remaining]\n",
        "                ATs = ATs[remaining]\n",
        "                ATys = ATys[remaining]\n",
        "                ATAs = ATAs[remaining]\n",
        "                sets = sets[remaining]\n",
        "                y = y[remaining]\n",
        "                r = r[remaining]\n",
        "                if problems_done.all():\n",
        "                    return result_sets, result_solutions, result_lengths\n",
        "        # GET PROJECTIONS AND INDICES TO ADD\n",
        "        if on_cpu:\n",
        "            projections = batch_mm(XT.numpy(), r[:, :, None].numpy())\n",
        "            argmax_blast(projections.squeeze(-1), sets[:, k].numpy())\n",
        "        else:\n",
        "            projections = XT @ r[:, :, None]\n",
        "            sets[:, k] = projections.abs().sum(-1).argmax(-1)  # Sum is just a squeeze, but would be relevant in SOMP.\n",
        "\n",
        "        # UPDATE AT\n",
        "        AT = ATs[:, :k + 1, :]\n",
        "        updateA = XT[sets[:, k], :]\n",
        "        AT[:, k, :] = updateA\n",
        "\n",
        "        # UPDATE ATy based on AT\n",
        "        ATy = ATys[:, :k + 1]\n",
        "        innerp(updateA, y, out=ATy[:, k, 0])\n",
        "\n",
        "        # UPDATE ATA based on AT or precomputed XTX.\n",
        "        if on_cpu:\n",
        "            packed_idx = k * (k - 1) // 2\n",
        "            if XTX is not None:  # Update based on precomputed XTX.\n",
        "                ATAs.t()[k + packed_idx:packed_idx + 2 * k + 1, :].t().numpy()[:] = XTX[sets[:, k, None], sets[:, :k + 1]]\n",
        "            else:\n",
        "                np.matmul(AT[:, :k + 1, :].numpy(), updateA[:, :, None].numpy(),\n",
        "                          out=ATAs.t()[k + packed_idx:packed_idx + 2 * k + 1, :].t()[:, :, None].numpy())\n",
        "        else:\n",
        "            ATA = ATAs[:, :k + 1, :k + 1]\n",
        "            if XTX is not None:\n",
        "                ATA[:, k, :k + 1] = XTX[sets[:, k, None], sets[:, :k + 1]]\n",
        "            else:\n",
        "                # Update ATAs by adding the new column of inner products.\n",
        "                torch.bmm(AT[:, :k + 1, :], updateA[:, :, None], out=ATA[:, k, :k + 1, None])\n",
        "\n",
        "        # SOLVE ATAx = ATy.\n",
        "        if on_cpu:\n",
        "            solutions = ATy.permute(0, 2, 1).clone().permute(0, 2, 1)  # Get a copy.\n",
        "            ppsv(ATAs.t()[:packed_idx + 2 * k + 1, :].t().contiguous().numpy(), solutions.numpy())\n",
        "        else:\n",
        "            ATA[:, :k, k] = ATA[:, k, :k]  # Copy lower triangle to upper triangle.\n",
        "            solutions = cholesky_solve(ATA, ATy)\n",
        "\n",
        "        # FINALLY, GET NEW RESIDUAL r=y-Ax\n",
        "        if on_cpu:\n",
        "            np.subtract(y.numpy(), (AT.permute(0, 2, 1).numpy() @ solutions.numpy()).squeeze(-1), out=r.numpy())\n",
        "        else:\n",
        "            torch.baddbmm(y[:, :, None], AT.permute(0, 2, 1), solutions, beta=-1, out=r[:, :, None])\n",
        "\n",
        "    return sets, solutions, None\n",
        "\n",
        "def omp_v0(X, y, XTX, n_nonzero_coefs=None, tol=None, inverse_cholesky=True):\n",
        "    B = y.shape[0]\n",
        "    normr2 = innerp(y)  # Norm squared of residual.\n",
        "    projections = (X.transpose(1, 0) @ y[:, :, None]).squeeze(-1)\n",
        "    sets = y.new_zeros(n_nonzero_coefs, B, dtype=torch.int64)\n",
        "\n",
        "    if inverse_cholesky:\n",
        "        # Doing the inverse-cholesky iteratively uses more memory,\n",
        "        # but takes less time than waiting till solving the problem in the end it seems.\n",
        "        # (Since F is triangular it could be __even faster__ to multiply, prob. not on GPU tho.)\n",
        "        F = torch.eye(n_nonzero_coefs, dtype=y.dtype, device=y.device).repeat(B, 1, 1)\n",
        "        a_F = y.new_zeros(n_nonzero_coefs, B, 1)\n",
        "\n",
        "    D_mybest = y.new_empty(B, n_nonzero_coefs, XTX.shape[0])\n",
        "    temp_F_k_k = y.new_ones((B, 1))\n",
        "\n",
        "    if tol:\n",
        "        result_lengths = sets.new_zeros(y.shape[0])\n",
        "        result_solutions = y.new_zeros((y.shape[0], n_nonzero_coefs, 1))\n",
        "        finished_problems = sets.new_zeros(y.shape[0], dtype=torch.bool)\n",
        "\n",
        "    for k in range(n_nonzero_coefs+bool(tol)):\n",
        "        # STOPPING CRITERIA\n",
        "        if tol:\n",
        "            problems_done = normr2 <= tol\n",
        "            if k == n_nonzero_coefs:\n",
        "                problems_done[:] = True\n",
        "\n",
        "            if problems_done.any():\n",
        "                new_problems_done = problems_done & ~finished_problems\n",
        "                finished_problems.logical_or_(problems_done)\n",
        "                result_lengths[new_problems_done] = k\n",
        "                if inverse_cholesky:\n",
        "                    result_solutions[new_problems_done, :k] = F[new_problems_done, :k, :k].permute(0, 2, 1) @ a_F[:k, new_problems_done].permute(1, 0, 2)\n",
        "                else:\n",
        "                    assert False, \"inverse_cholesky=False with tol != None is not handled yet\"\n",
        "                if problems_done.all():\n",
        "                    return sets.t(), result_solutions, result_lengths\n",
        "\n",
        "        sets[k] = projections.abs().argmax(1)\n",
        "        # D_mybest[:, k, :] = XTX[gamma[k], :]  # Same line as below, but significantly slower. (prob. due to the intermediate array creation)\n",
        "        torch.gather(XTX, 0, sets[k, :, None].expand(-1, XTX.shape[1]), out=D_mybest[:, k, :])\n",
        "        if k:\n",
        "            D_mybest_maxindices = D_mybest.permute(0, 2, 1)[torch.arange(D_mybest.shape[0], dtype=sets.dtype, device=sets.device), sets[k], :k]\n",
        "            torch.rsqrt(1 - innerp(D_mybest_maxindices),\n",
        "                        out=temp_F_k_k[:, 0])  # torch.exp(-1/2 * torch.log1p(-inp), temp_F_k_k[:, 0])\n",
        "            D_mybest_maxindices *= -temp_F_k_k  # minimal operations, exploit linearity\n",
        "            D_mybest[:, k, :] *= temp_F_k_k\n",
        "            D_mybest[:, k, :, None].baddbmm_(D_mybest[:, :k, :].permute(0, 2, 1), D_mybest_maxindices[:, :, None])\n",
        "\n",
        "\n",
        "        temp_a_F = temp_F_k_k * torch.gather(projections, 1, sets[k, :, None])\n",
        "        normr2 -= (temp_a_F * temp_a_F).squeeze(-1)\n",
        "        projections -= temp_a_F * D_mybest[:, k, :]\n",
        "        if inverse_cholesky:\n",
        "            a_F[k] = temp_a_F\n",
        "            if k:  # Could maybe get a speedup from triangular mat mul kernel.\n",
        "                torch.bmm(D_mybest_maxindices[:, None, :], F[:, :k, :], out=F[:, k, None, :])\n",
        "                F[:, k, k] = temp_F_k_k[..., 0]\n",
        "    else: # FIXME: else branch will not execute if n_nonzero_coefs=0, so solutions is undefined.\n",
        "        # Normal exit, used when tolerance=None.\n",
        "        if inverse_cholesky:\n",
        "            solutions = F.permute(0, 2, 1) @ a_F.squeeze(-1).transpose(1, 0)[:, :, None]\n",
        "        else:\n",
        "            # Solving the problem in the end without using inverse Cholesky.\n",
        "            AT = X.T[sets.T]\n",
        "            solutions = cholesky_solve(AT @ AT.permute(0, 2, 1), AT @ y.T[:, :, None])\n",
        "\n",
        "    return sets.t(), solutions, None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRHA7INejuWz"
      },
      "source": [
        "# Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIvbH9ewrBbu",
        "outputId": "4c97e828-99b0-421e-b5e3-716e8e304ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 32.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 63.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
            "Mounted at /content/drive\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "STATE_PATH = '/content/drive/MyDrive/NLP-Final/bert_state_w_finetuning'\n",
        "KEYS_VALUES_PATH = \"/content/drive/My Drive/NLP-Final/bert_sparse_keys&values/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKthqrfspFXf"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/NLP-Final/'\n",
        "\n",
        "train_data = pd.read_csv(path + 'train.csv')\n",
        "test_data = pd.read_csv(path + 'test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG6l9XH-qw-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9a2cb7-a407-43bc-ca12-1b9dca31ec58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2000, 500, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_data = train_data[:2000]\n",
        "test_data = test_data[:500]\n",
        "\n",
        "train_data = train_data.to_dict(orient='records')\n",
        "test_data = test_data.to_dict(orient='records')\n",
        "type(train_data)\n",
        "\n",
        "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n",
        "\n",
        "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhIN1TIosy1W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "166b1b75bc9c4da3870deca4d4451f4e",
            "49b86b4abe264409aa0334b206d226ac",
            "51cae743affe466492e567e468e7b5c4",
            "09e53fc1789e470ea14bde6d4d4d3464",
            "666fbffb859c4d8e9deb05d9dfd8e50b",
            "888f176bc0fa44b9aca9517081c75f8e",
            "e65132124d1b4bcbb8ed64d5b7abe923",
            "8ad83f1c594042089fce60f63b4c292d",
            "459f4b02bd804165ace138f8170fbdf9",
            "d96a8dd9e0b84737929d35c648153b31",
            "6d7ce69aca284e2f977b3e2e80239462",
            "970cefb5300b4005a3557d1f25daacc6",
            "f80796b54f8c4a6a81408ce977c2266a",
            "37938b0caba9490f92762c9658a0d785",
            "5eac9a4c1cd24d8db26a3f1cbadf72a1",
            "f030bca31dfa42e7abbcb640944ccb66",
            "ded8adf558494410b42ed733f4820acb",
            "0f6366a8a68a4533a20400535d19009d",
            "7ca03612b6ea45d4bc630f26894caa17",
            "57f18d4ca11248daa7d5c5954e80e5f0",
            "0506499e3197427abdc87a0aefa20119",
            "3184bad1dc2e43f1a0f9fb3c24402b43",
            "c9289c171a6a46e9a64352cebe6ecaab",
            "9fd11ef7b9304497b0bff839b270c209",
            "c4ed52c18bdb4bbfad24fd7bf9e25a12",
            "b75f16a0d7564a43bc9e789cbfac7ed6",
            "f9426698e6c94a76915a91f9ca14f23f",
            "fb2ff1fc65a24a8dbf62537c23e3b9b9",
            "a67915d419b94295b30b9efcf5fc2b96",
            "43365a61508a46c589c184451af63601",
            "a275beec70d74529bda89704988ffd3c",
            "61442e598a0f40de89c1c71a1ba3c3dc",
            "5c8d9dc61a2f47e58ab01b504d0415c7"
          ]
        },
        "outputId": "069df47c-4a46-4c23-87a7-656e97ea76db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "166b1b75bc9c4da3870deca4d4451f4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "970cefb5300b4005a3557d1f25daacc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9289c171a6a46e9a64352cebe6ecaab"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n",
        "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "\n",
        "train_y = np.array(train_labels) == 'pos'\n",
        "test_y = np.array(test_labels) == 'pos'\n",
        "\n",
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(test_y, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "metadata": {
        "id": "NX-j-KDxbiMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4876eac0-af6a-4ce6-a28c-b06d1e40de60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{False: 246, True: 254}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_DWoyuX7vBjC"
      },
      "outputs": [],
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        output = self.bert(tokens, attention_mask=masks)\n",
        "        pooled_output = output.pooler_output\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIFgyz1iwBG6"
      },
      "outputs": [],
      "source": [
        "# bert_clf = BertBinaryClassifier()\n",
        "# bert_clf = bert_clf.cuda()\n",
        "# x = torch.tensor(train_tokens_ids[:3]).to(device)\n",
        "# y, pooled = bert_clf.bert(x)\n",
        "# y = bert_clf(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5UDUU7vyrTt"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "\n",
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)\n",
        "\n",
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctRm2dN33r_M"
      },
      "outputs": [],
      "source": [
        "def train_model(model):\n",
        "  param_optimizer = list(model.sigmoid.named_parameters()) \n",
        "  optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "  optimizer = Adam(model.parameters(), lr=3e-6)\n",
        "  torch.cuda.empty_cache()   # Clearing Cache space for a fresh Model run\n",
        "\n",
        "  for epoch_num in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "        print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
        "        logits = model(token_ids, masks)\n",
        "        \n",
        "        loss_func = nn.BCELoss()\n",
        "\n",
        "        batch_loss = loss_func(logits, labels)\n",
        "        train_loss += batch_loss.item()\n",
        "        \n",
        "        model.zero_grad()\n",
        "        batch_loss.backward()\n",
        "\n",
        "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWz7U6X2ydWY"
      },
      "outputs": [],
      "source": [
        "def eval_model(model):\n",
        "  model.eval().to(device)\n",
        "  model_predicted = []\n",
        "  all_logits = []\n",
        "  with torch.no_grad():\n",
        "      for step_num, batch_data in enumerate(test_dataloader):\n",
        "          token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "          logits = model(token_ids, masks)\n",
        "          loss_func = nn.BCELoss()\n",
        "          loss = loss_func(logits, labels)\n",
        "          numpy_logits = logits.cpu().detach().numpy()\n",
        "          \n",
        "          model_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "          all_logits += list(numpy_logits[:, 0])\n",
        "\n",
        "          token_ids, masks, labels = tuple(t.to(\"cpu\") for t in batch_data) # PATCH- move back to cpu\n",
        "\n",
        "  \n",
        "  report = classification_report(test_y, model_predicted, output_dict=True)\n",
        "  model.to(\"cpu\")\n",
        "  return report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG03F5BP4qUj"
      },
      "outputs": [],
      "source": [
        "def convert_model_state_to_zeros(model, model_state):\n",
        "  # print(st)\n",
        "  for name,x in model.named_parameters():\n",
        "  #  if \"intermediate.dense\" in name  or \"output.dense\" in name:\n",
        "  # if \"intermediate.dense\" in name or \"output.dense\" in name: # intermidiate is the feed forward part of the layer \n",
        "      model_state[name]  = model_state[name]*0\n",
        "  return model_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3j0Yugew3lm"
      },
      "outputs": [],
      "source": [
        "def print_layers_params(model, model_state):\n",
        "  # print(st)\n",
        "  for name,x in model.named_parameters():\n",
        "  #  if \"intermediate.dense\" in name  or \"output.dense\" in name:\n",
        "    if \"intermediate.dense\" in name: # intermidiate is the feed forward part of the layer \n",
        "      print(model_state[name])\n",
        "\n",
        "# print_layers_params(bert_clf, bert_clf.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse-Keys Analysis"
      ],
      "metadata": {
        "id": "ELnxxFWS2s7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import savetxt\n",
        "LAYER_NUM = 11\n",
        "\n",
        "# Load fine-tuned BERT\n",
        "bert_sparse = BertBinaryClassifier()\n",
        "bert_sparse = bert_sparse.cuda()\n",
        "bert_sparse.load_state_dict(torch.load(STATE_PATH))\n",
        "\n",
        "# Get embeddings and BERT state\n",
        "embeddings = bert_sparse.bert.get_input_embeddings()\n",
        "E = embeddings.weight.detach().to(device)\n",
        "bert1_st = bert_sparse.bert.state_dict() # Bert model state dictionary\n",
        "\n",
        "# Compute sparse keys\n",
        "keys = bert1_st[f\"encoder.layer.{LAYER_NUM}.intermediate.dense.weight\"].detach().to(device)\n",
        "n_non_zero_keys = max(int(0.3 * keys.shape[0]), 1)\n",
        "sparse_keys = run_omp(E.T,  keys, n_non_zero_keys) # run optimized OMP\n",
        "savetxt(KEYS_VALUES_PATH+f\"layer{LAYER_NUM}_keys.csv\", sparse_keys, delimiter=',')\n",
        "\n",
        "# Compute sparse values\n",
        "values = bert1_st[f\"encoder.layer.{LAYER_NUM}.output.dense.weight\"].detach().to(device)\n",
        "n_non_zero_values = max(int(0.3 * values.shape[1]), 1)\n",
        "sparse_values = run_omp(E.T,  values.T, n_non_zero_values) # run optimized OMP\n",
        "savetxt(KEYS_VALUES_PATH+f\"layer{LAYER_NUM}_values.csv\", sparse_keys, delimiter=',')"
      ],
      "metadata": {
        "id": "6y_OD1X_2pKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import loadtxt\n",
        "LAYER_NUM = 11\n",
        "\n",
        "sparse_keys = loadtxt(KEYS_VALUES_PATH+f\"layer{LAYER_NUM}_keys.csv\", delimiter=',').astype(\"double\")\n",
        "sparse_keys = torch.from_numpy(sparse_keys).to(device)\n",
        "sparse_values = loadtxt(KEYS_VALUES_PATH+f\"layer{LAYER_NUM}_values.csv\", delimiter=',').astype(\"double\")\n",
        "sparse_values = torch.from_numpy(sparse_values).to(device)"
      ],
      "metadata": {
        "id": "1XYP0UVr4Mhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "for i in range(100):\n",
        "  # non_zero_keys = torch.nonzero(sparse_keys[i])\n",
        "  sorted_keys, indices_keys = torch.sort(torch.abs(sparse_keys[i]), descending=True)\n",
        "  print(tokenizer.batch_decode(indices_keys[0:10]))\n",
        "\n",
        "  # non_zero_values = torch.nonzero(sparse_values[i])\n",
        "  sorted_values, indices_values = torch.sort(torch.abs(sparse_values[i]), descending=True)\n",
        "  print(tokenizer.batch_decode(indices_values[0:10]))\n",
        "  # print(tokenizer.batch_decode(non_zero_values))\n",
        "  print(\"#################################################\")"
      ],
      "metadata": {
        "id": "6Xh3BOhwPMqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import loadtxt\n",
        "\n",
        "layer_num = 11\n",
        "\n",
        "# load the sparse keys and values, that were computed by the OMP\n",
        "sparse_keys = loadtxt(KEYS_VALUES_PATH+f\"layer{layer_num}_keys.csv\", delimiter=',').astype(\"double\")\n",
        "sparse_keys = torch.from_numpy(sparse_keys)\n",
        "sparse_values = loadtxt(KEYS_VALUES_PATH+f\"layer{layer_num}_values.csv\", delimiter=',').astype(\"double\")\n",
        "sparse_values = torch.from_numpy(sparse_values)\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "for i in range(100):\n",
        "  sorted_keys, indices_keys = torch.sort(torch.abs(sparse_keys[i]), descending=True)\n",
        "  print(bert_tokenizer.batch_decode(indices_keys[0:10]))\n",
        "\n",
        "  sorted_values, indices_values = torch.sort(torch.abs(sparse_values[i]), descending=True)\n",
        "  print(bert_tokenizer.batch_decode(indices_values[0:10]))\n",
        "  print(\"#################################################\")"
      ],
      "metadata": {
        "id": "GxuB3ylhWEZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #####RUN ALL BEFORE THIS LINE !! - CLICK \"RUN BEFORE\" ON THIS CELL //Ctrl+F8\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iU8p-XsfEnN-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mopvGI8tD6tB"
      },
      "source": [
        "# Experiment - No Need for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvRxoZ1G4vTe",
        "outputId": "6950fc2e-4d33-4086-cd43-b35ad9a08adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0101, -0.0017,  0.0130,  ..., -0.0537,  0.0137, -0.0192],\n",
            "        [-0.0604,  0.0104,  0.0648,  ..., -0.0486, -0.0277,  0.0070],\n",
            "        [-0.0147,  0.0428,  0.0398,  ..., -0.0082,  0.0114, -0.0275],\n",
            "        ...,\n",
            "        [-0.0500, -0.0660,  0.0270,  ..., -0.0131,  0.0041,  0.0068],\n",
            "        [ 0.0448, -0.0237,  0.0167,  ...,  0.0035,  0.0069,  0.0205],\n",
            "        [-0.0094,  0.0363,  0.0258,  ..., -0.0086,  0.0064,  0.0604]])\n"
          ]
        }
      ],
      "source": [
        "# regular eval\n",
        "bert_regular = BertBinaryClassifier()\n",
        "# bert_regular = bert_regular.cuda()\n",
        "regular_st = bert_regular.state_dict()\n",
        "print(regular_st[\"bert.encoder.layer.0.intermediate.dense.weight\"])\n",
        "\n",
        "# model= train_model(bert_regular)\n",
        "# regular_st = bert_regular.state_dict()\n",
        "# print(regular_st[\"bert.encoder.layer.0.intermediate.dense.weight\"])\n",
        "\n",
        "# eval_model(bert_regular)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjbba9cA8lyO"
      },
      "outputs": [],
      "source": [
        "# # save BERT's state after finetuning\n",
        "# STATE_PATH = '/content/drive/MyDrive/NLP-Final/bert_state_w_finetuning'\n",
        "# regular_st = bert_regular.state_dict()\n",
        "# torch.save(regular_st, STATE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntTuBRdkCNW7"
      },
      "outputs": [],
      "source": [
        "# bert_check = BertBinaryClassifier()\n",
        "# bert_check = bert_check.cuda()\n",
        "# bert_check.load_state_dict(torch.load(STATE_PATH))\n",
        "# eval_model(bert_check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUuoKPRL_ruo"
      },
      "outputs": [],
      "source": [
        "# # convert ff params to zeros and eval\n",
        "# bert_zeros = BertBinaryClassifier()\n",
        "# bert_zeros = bert_zeros.cuda()\n",
        "# zeros_st = convert_model_state_to_zeros(bert_zeros, bert_zeros.state_dict())\n",
        "# bert_zeros.load_state_dict(zeros_st)\n",
        "# zeros_st = bert_zeros.state_dict()\n",
        "# print(zeros_st[\"bert.encoder.layer.0.intermediate.dense.weight\"])\n",
        "# print(zeros_st[\"bert.encoder.layer.0.output.dense.weight\"])\n",
        "# eval_model(bert_zeros) # eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzVT5tI_YWCI"
      },
      "source": [
        "# Sparsity with SKlearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81IK3eQHJTtM",
        "outputId": "2ba2ddef-4323-4934-b3cb-80748c795f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
            "  X, y[:, k], n_nonzero_coefs, tol, copy_X=copy_X, return_path=return_path\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-3bd05b1ec033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mn_non_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrthogonalMatchingPursuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_omp.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    751\u001b[0m                 \u001b[0mprecompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0mcopy_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m                 \u001b[0mreturn_n_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             )\n\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_omp.py\u001b[0m in \u001b[0;36morthogonal_mp\u001b[0;34m(X, y, n_nonzero_coefs, tol, precompute, copy_X, return_path, return_n_iter)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         out = _cholesky_omp(\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_X\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         )\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_omp.py\u001b[0m in \u001b[0;36m_cholesky_omp\u001b[0;34m(X, y, n_nonzero_coefs, tol, copy_X, return_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_active\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_float\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;31m# atom already selected or inner product too small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "bert_sparse = BertBinaryClassifier()\n",
        "bert_sparse = bert_sparse.cuda()\n",
        "bert_sparse.load_state_dict(torch.load(STATE_PATH))\n",
        "# eval_model(bert_sparse)\n",
        "\n",
        "# find x that minimize ||keys - Embeddings*x||, given sparsity level for x\n",
        "# meaning, the best approx for: keys = E*x\n",
        "# replace this x with original keys\n",
        "emb = bert_sparse.bert.get_input_embeddings()\n",
        "bert_sparse_st = bert_sparse.bert.state_dict()\n",
        "\n",
        "X = emb.weight.cpu().detach().numpy()\n",
        "y = bert_sparse_st[\"encoder.layer.0.intermediate.dense.weight\"].cpu().detach().numpy()\n",
        "print(type(X))\n",
        "print(type(y))\n",
        "\n",
        "n_features = X.shape[1]\n",
        "n_non_zero = max(int(0.1 * n_features), 1)\n",
        "\n",
        "mp = OrthogonalMatchingPursuit(normalize=False).fit(X.T, y.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ggwNldrYRtp"
      },
      "outputs": [],
      "source": [
        "# save mp!!!\n",
        "print(mp)\n",
        "print(mp.size())\n",
        "\n",
        "# Replace! + save mp!\n",
        "bert_sparse_st[\"encoder.layer.0.intermediate.dense.weight\"] = mp\n",
        "bert_sparse.bert.load_state_dict(bert_sparse_st)\n",
        "bert_sparse_st = bert_sparse.state_dict()\n",
        "torch.save(bert_sparse_st, '/content/drive/MyDrive/NLP-Final/bert_state_w_mp_0.1')\n",
        "eval_model(bert_sparse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa-ia6e8EPIr"
      },
      "source": [
        "# Sparsity with OMP - DEV and CHECK"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert1 = BertBinaryClassifier()\n",
        "bert1.load_state_dict(torch.load(STATE_PATH))\n"
      ],
      "metadata": {
        "id": "Ym64bTXE19FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GpimdBKk1d0"
      },
      "outputs": [],
      "source": [
        "bert1 = BertBinaryClassifier()\n",
        "bert1 = bert1.cuda()\n",
        "bert1.load_state_dict(torch.load(STATE_PATH))\n",
        "\n",
        "emb1 = bert1.bert.get_input_embeddings()\n",
        "bert1_st = bert1.bert.state_dict()\n",
        "\n",
        "X = emb1.weight.cpu().detach().to(device)\n",
        "y = bert1_st[\"encoder.layer.0.attention.output.dense.weight\"].detach().to(device)\n",
        "\n",
        "n_features = X.shape[1]\n",
        "n_non_zero = max(int(0.1 * n_features), 1)\n",
        "\n",
        "xes = run_omp(X.T,  y.T, n_non_zero,alg='v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHKpuGRVsQ3x"
      },
      "outputs": [],
      "source": [
        "xes.size()\n",
        "bert1_st[\"encoder.layer.0.attention.output.dense.weight\"] = xes\n",
        "bert1.bert.load_state_dict(bert1_st)\n",
        "eval_model(bert1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = bert1_st[\"encoder.layer.2.intermediate.dense.weight\"].cpu().numpy()\n"
      ],
      "metadata": {
        "id": "6RpYwNtULC3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "mp = OrthogonalMatchingPursuit(normalize=False).fit(X.T.cpu(), y.T)\n"
      ],
      "metadata": {
        "id": "YZQF4z5oKsVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no replace\n",
        "eval_model(bert1)"
      ],
      "metadata": {
        "id": "evtJJg5mxBp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# layer 2 intermediate replace\n",
        "j = mp.predict(X.T.cpu())\n",
        "bert1_st[\"encoder.layer.2.intermediate.dense.weight\"]  = torch.Tensor(j.T)\n",
        "bert1.bert.load_state_dict(bert1_st)\n",
        "eval_model(bert1)"
      ],
      "metadata": {
        "id": "Myd6U7FUb_9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(bert1, '/content/drive/MyDrive/NLP-Final/bert_state_layer2_intermediate_sparse')\n",
        "# torch.save(bert1, '/content/drive/MyDrive/NLP-Final/bert_state_layer0_attention_output_dense_sparse')\n"
      ],
      "metadata": {
        "id": "X9jjSRm7olLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attention output replace\n",
        "j = mp.predict(X.T.cpu())\n",
        "bert1_st[\"encoder.layer.0.attention.output.dense.weight\"]  = torch.Tensor(j.T)\n",
        "bert1.bert.load_state_dict(bert1_st)\n",
        "eval_model(bert1)"
      ],
      "metadata": {
        "id": "sDxnQZMkY02q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqgZKLP7QOrS"
      },
      "outputs": [],
      "source": [
        "# bert1.to(\"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "emb1 = bert1.bert.get_input_embeddings()\n",
        "bert1_st = bert1.bert.state_dict()\n",
        "\n",
        "X_tensor = emb1.weight.detach().to(device)\n",
        "y_tensor = bert1_st[\"encoder.layer.2.intermediate.dense.weight\"].detach().to(device)\n",
        "\n",
        "num_features = X_tensor.shape[1]\n",
        "n_non_zero = max(int(0.1 * num_features), 1)\n",
        "\n",
        "# xes = run_omp(X_tensor.T.cpu(),  y_tensor.cpu(), n_non_zero, alg='v0')\n",
        "xes = run_omp(X_tensor.T,  y_tensor, n_non_zero)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(xes @ X_tensor).shape"
      ],
      "metadata": {
        "id": "He0aaUv66j2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# layer 2 intermediate replace - OMP FAST\n",
        "j = xes @ X_tensor\n",
        "bert1_st[\"encoder.layer.2.intermediate.dense.weight\"]  = j\n",
        "bert1.bert.load_state_dict(bert1_st)\n",
        "eval_model(bert1)"
      ],
      "metadata": {
        "id": "vErmiZr97XUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #####Run only one of the sections Below due to GPU Memory Runout"
      ],
      "metadata": {
        "id": "cPDY2YliLJCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#10% of Coefficients are Non-zero - TESTS"
      ],
      "metadata": {
        "id": "u4F6aWkzD8ON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT Test - Replace each layer independently**\n",
        "\n",
        "```\n",
        "    bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "    y_tensor = bert1_st[f\"encoder.layer.{i-1}.intermediate.dense.weight\"].detach().to(device) # get weights for ith ff layer\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "AlANW1sFC6YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace every state untouching the others\n",
        "emb1 = bert1.bert.get_input_embeddings() # embeddings\n",
        "bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "\n",
        "X_tensor = emb1.weight.detach().to(device) \n",
        "num_features = X_tensor.shape[1]\n",
        "n_non_zero = max(int(0.1 * num_features), 1) # 10% of X num of features\n",
        "\n",
        "exp_results = dict()\n",
        "\n",
        "\n",
        "for i in range(13):\n",
        "  torch.cuda.empty_cache() # Clear memory\n",
        "  bert1.load_state_dict(torch.load(STATE_PATH)) # load first the original weights\n",
        "\n",
        "  if i == 0:\n",
        "    print(\"Results Before Replacement\")\n",
        "    res = eval_model(bert1)\n",
        "    exp_results[f'No_Replace'] = res\n",
        "    continue\n",
        "\n",
        "  bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "  y_tensor = bert1_st[f\"encoder.layer.{i-1}.intermediate.dense.weight\"].detach().to(device) # get weights for ith ff layer\n",
        "\n",
        "  xes = run_omp(X_tensor.T,  y_tensor, n_non_zero) # run fast OMP\n",
        "\n",
        "  temp_weights = xes @ X_tensor # matrix mult to get the weights\n",
        "\n",
        "  bert1_st[f\"encoder.layer.{i-1}.intermediate.dense.weight\"]  = temp_weights\n",
        "  bert1.bert.load_state_dict(bert1_st)\n",
        "  print(f\"Results for layer-{i-1} Weights Replacement\")\n",
        "  res = eval_model(bert1)\n",
        "  exp_results[f'layer-{i-1}'] = res\n",
        "\n",
        "\n",
        "  y_tensor.to(\"cpu\")\n",
        "\n",
        "# pd.DataFrame.from_dict(exp_results).to_excel(f'/content/drive/MyDrive/NLP-Final/experiments_results/BERT/Replace_each_layer_leaving_others_untouched.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sioqAT8zCryd",
        "outputId": "57a99d95-edd3-4b1b-c4d7-65a8857a2f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results Before Replacement\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
            "L = torch.cholesky(A)\n",
            "should be replaced with\n",
            "L = torch.linalg.cholesky(A)\n",
            "and\n",
            "U = torch.cholesky(A, upper=True)\n",
            "should be replaced with\n",
            "U = torch.linalg.cholesky(A).transpose(-2, -1).conj().\n",
            "This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1285.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for layer-0 Weights Replacement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot the keys as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(temp_weights.cpu(), bins=100)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "aIiBoAR-toiX",
        "outputId": "1d71c5e7-859d-441e-a3f3-30a983b83c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count vector frequencies\n",
        "k = np.zeros(temp_weights.shape[1])\n",
        "for index in temp_weights.nonzero():\n",
        "  k[index[1]] += 1\n"
      ],
      "metadata": {
        "id": "GtBG0WQ02RpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_weights.nonzero().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv08_K-i_iQ-",
        "outputId": "a08872cf-e73e-481d-ad38-455cbb1701e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2359296, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT Test - Replace layers one by one together**\n",
        "\n",
        "```\n",
        "    y_tensor = bert1_st[f\"encoder.layer.{i-1}.intermediate.dense.weight\"].detach().to(device) # get weights for ith ff layer\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "fWaZn2XjDXL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Replace layers one by one together\n",
        "emb1 = bert1.bert.get_input_embeddings() # embeddings\n",
        "bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "\n",
        "X_tensor = emb1.weight.detach().to(device) \n",
        "num_features = X_tensor.shape[1]\n",
        "\n",
        "ratios = [0.01,0.02,0.05,0.1,0.15,0.2,0.25]\n",
        "for j in ratios:\n",
        "  n_non_zero = max(int(j * num_features), 1)+1 # 10% of X num of features\n",
        "\n",
        "  print(f'{100*j:.2f}% Sparsity - {n_non_zero} coef.')\n",
        "  exp_results = dict()\n",
        "  for i in range(13):\n",
        "    torch.cuda.empty_cache() # Clear memory\n",
        "    bert1.load_state_dict(torch.load(STATE_PATH)) # load first the original weights\n",
        "\n",
        "    if i == 0:\n",
        "      print(\"Results Before Replacement\")\n",
        "      res = eval_model(bert1)\n",
        "      exp_results[f'No_Replace'] = res\n",
        "      continue\n",
        "\n",
        "    # bert1_st is updated every step\n",
        "    y_tensor = bert1_st[f\"encoder.layer.{i-1}.intermediate.dense.weight\"].detach().to(device) # get weights for ith ff layer\n",
        "\n",
        "    xes = run_omp(X_tensor.T,  y_tensor, n_non_zero) # run fast OMP\n",
        "\n",
        "    temp_weights = xes @ X_tensor # matrix mult to get the weights\n",
        "\n",
        "    bert1_st[f\"encoder.layer.{i-1}.intermediate.dense.weight\"]  = temp_weights\n",
        "    bert1.bert.load_state_dict(bert1_st)\n",
        "    print(f\"Results for Replacing all the layers up to layer-{i-1} (included) \")\n",
        "    res = eval_model(bert1)\n",
        "    exp_results[f'layer0_to_{i}'] = res\n",
        "\n",
        "  pd.DataFrame.from_dict(exp_results).to_excel(f'/content/drive/MyDrive/NLP-Final/experiments_results/BERT/Replace_layers_one_by_one_{100*j:.2f}_percent_{n_non_zero}_coef.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a2o0TSXg7zwr",
        "outputId": "3b580cd1-ba5b-4582-caa0-912cce9e265f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.00% Sparsity - 8 coef.\n",
            "Results Before Replacement\n",
            "Results for Replacing all the layers up to layer-0 (included) \n",
            "Results for Replacing all the layers up to layer-1 (included) \n",
            "Results for Replacing all the layers up to layer-2 (included) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Replacing all the layers up to layer-3 (included) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Replacing all the layers up to layer-4 (included) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Replacing all the layers up to layer-5 (included) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Replacing all the layers up to layer-6 (included) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Replacing all the layers up to layer-7 (included) \n",
            "Results for Replacing all the layers up to layer-8 (included) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Replacing all the layers up to layer-9 (included) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Replacing all the layers up to layer-10 (included) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Replacing all the layers up to layer-11 (included) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.00% Sparsity - 16 coef.\n",
            "Results Before Replacement\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2dfbbc6db3ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0my_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert1_st\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"encoder.layer.{i-1}.intermediate.dense.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get weights for ith ff layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_omp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_non_zero\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run fast OMP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtemp_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxes\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX_tensor\u001b[0m \u001b[0;31m# matrix mult to get the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-30b01b536fcf>\u001b[0m in \u001b[0;36mrun_omp\u001b[0;34m(X, y, n_nonzero_coefs, precompute, tol, normalize, fit_intercept, alg)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# If n_nonzero_coefs is equal to M, one should just return lstsq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'naive'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0msets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momp_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'v0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0msets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momp_v0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-30b01b536fcf>\u001b[0m in \u001b[0;36momp_naive\u001b[0;34m(X, y, n_nonzero_coefs, tol, XTX)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Copy lower triangle to upper triangle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0msolutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcholesky_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mATy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# FINALLY, GET NEW RESIDUAL r=y-Ax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-30b01b536fcf>\u001b[0m in \u001b[0;36mcholesky_solve\u001b[0;34m(ATA, ATy)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mATy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mATy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mATy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cholesky: (Batch element 0): The factorization could not be completed because the input is not positive-definite (the leading minor of order 9 is not positive-definite)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Different Sparsity Levels - Run Alone - No GPU Memory for all the EXP together - TESTS"
      ],
      "metadata": {
        "id": "POS_ksPJJkrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT Test - Replace only for layer 0 and change num of Non Zero Coefficients**\n",
        "\n",
        "\n",
        "```\n",
        "  # Change ratio of non_zero_coef each iteration\n",
        "  n_non_zero = max(int(ratio * num_features), 1) # ratio of X num of features\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "V5iOxfYvH1Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1%-45%\n",
        "import gc \n",
        "emb1 = bert1.bert.get_input_embeddings() # embeddings\n",
        "bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "\n",
        "X_tensor = emb1.weight.detach().to(device) \n",
        "num_features = X_tensor.shape[1]\n",
        "\n",
        "\n",
        "ratios = [0.0,0.01,0.05,0.1,0.15,0.2,0.3,0.35,0.4,0.45]\n",
        "\n",
        "for j in range (12):\n",
        "  gc.collect()\n",
        "  exp_results = dict()\n",
        "  for i, ratio in enumerate(ratios):\n",
        "    # Change ratio of non_zero_coef each iteration\n",
        "    n_non_zero = max(int(ratio * num_features), 1) # ratio of X num of features\n",
        "    torch.cuda.empty_cache() # Clear memory\n",
        "    \n",
        "    bert1.load_state_dict(torch.load(STATE_PATH)) # load first the original weights\n",
        "\n",
        "    if i == 0: # print baseline results first\n",
        "      print(\"Results Before Replacement\")\n",
        "      eval_model(bert1)\n",
        "      continue\n",
        "\n",
        "    bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "    y_tensor = bert1_st[f\"encoder.layer.{j}.intermediate.dense.weight\"].detach().to(device) # get weights for ith ff layer\n",
        "\n",
        "    xes = run_omp(X_tensor.T,  y_tensor, n_non_zero) # run fast OMP\n",
        "\n",
        "    temp_weights = xes @ X_tensor # matrix mult to get the weights\n",
        "\n",
        "    bert1_st[f\"encoder.layer.{j}.intermediate.dense.weight\"]  = temp_weights\n",
        "    bert1.bert.load_state_dict(bert1_st)\n",
        "    print(f\"Results for layer-{j} Weights Replacement with {100*ratio:.2f}% Non-Zero Coefficients\")\n",
        "    res = eval_model(bert1)\n",
        "    exp_results[f'{100*ratio:.2f}%'] = res\n",
        "    y_tensor.to(\"cpu\")\n",
        "\n",
        "  pd.DataFrame.from_dict(exp_results).to_excel(f'/content/drive/MyDrive/NLP-Final/experiments_results/BERT/Sparsity_Levels_1_45_layer{j}.xlsx')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "uwd200vV-oMo",
        "outputId": "072760d7-f7e3-4ec3-92eb-31fb85244263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results Before Replacement\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-1bb2a4403acb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# print baseline results first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results Before Replacement\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f0c98039924a>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m           \u001b[0mnumpy_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mmodel_predicted\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 40%-60%\n",
        "emb1 = bert1.bert.get_input_embeddings() # embeddings\n",
        "bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "\n",
        "X_tensor = emb1.weight.detach().to(device) \n",
        "num_features = X_tensor.shape[1]\n",
        "\n",
        "for i, ratio in enumerate(np.linspace(0.3, 0.6, 4)):\n",
        "  bert1.to(\"cpu\")  \n",
        "  # Change ratio of non_zero_coef each iteration\n",
        "  n_non_zero = max(int(ratio * num_features), 1) # ratio of X num of features\n",
        "  torch.cuda.empty_cache() # Clear memory\n",
        "  \n",
        "  bert1.load_state_dict(torch.load(STATE_PATH)) # load first the original weights\n",
        "\n",
        "  if i == 0: # print baseline results first\n",
        "    print(\"Results Before Replacement\")\n",
        "    eval_model(bert1)\n",
        "    continue\n",
        "\n",
        "  bert1_st = bert1.bert.state_dict() # Bert model state dictionary\n",
        "  y_tensor = bert1_st[f\"encoder.layer.0.intermediate.dense.weight\"].detach().to(device) # get weights for ith ff layer\n",
        "\n",
        "  xes = run_omp(X_tensor.T,  y_tensor, n_non_zero) # run fast OMP\n",
        "\n",
        "  temp_weights = xes @ X_tensor # matrix mult to get the weights\n",
        "\n",
        "  bert1_st[f\"encoder.layer.0.intermediate.dense.weight\"]  = temp_weights\n",
        "  bert1.bert.load_state_dict(bert1_st)\n",
        "  print(f\"/content/drive/MyDrive/NLP-Final/experiments_results/BERT/Results for layer-0 Weights Replacement with {100*ratio:.2f}% Non-Zero Coefficients\")\n",
        "  eval_model(bert1)\n",
        "\n",
        "  y_tensor.to(\"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "id": "VsQabtdmKans",
        "outputId": "1e95e102-7e09-4a4d-c973-f74d7cfd8204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results Before Replacement\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.93      0.90      0.91       246\n",
            "        True       0.91      0.93      0.92       254\n",
            "\n",
            "    accuracy                           0.92       500\n",
            "   macro avg       0.92      0.92      0.92       500\n",
            "weighted avg       0.92      0.92      0.92       500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
            "L = torch.cholesky(A)\n",
            "should be replaced with\n",
            "L = torch.linalg.cholesky(A)\n",
            "and\n",
            "U = torch.cholesky(A, upper=True)\n",
            "should be replaced with\n",
            "U = torch.linalg.cholesky(A).transpose(-2, -1).conj().\n",
            "This transform will produce equivalent results for all valid (symmetric positive definite) inputs. (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1285.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for layer-0 Weights Replacement with 40.00% Non-Zero Coefficients\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.92      0.89      0.90       246\n",
            "        True       0.90      0.93      0.91       254\n",
            "\n",
            "    accuracy                           0.91       500\n",
            "   macro avg       0.91      0.91      0.91       500\n",
            "weighted avg       0.91      0.91      0.91       500\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-718210305bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0my_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert1_st\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"encoder.layer.0.intermediate.dense.weight\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get weights for ith ff layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_omp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_non_zero\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run fast OMP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mtemp_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxes\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX_tensor\u001b[0m \u001b[0;31m# matrix mult to get the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-30b01b536fcf>\u001b[0m in \u001b[0;36mrun_omp\u001b[0;34m(X, y, n_nonzero_coefs, precompute, tol, normalize, fit_intercept, alg)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# If n_nonzero_coefs is equal to M, one should just return lstsq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'naive'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0msets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momp_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'v0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0msets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momp_v0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_nonzero_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXTX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-30b01b536fcf>\u001b[0m in \u001b[0;36momp_naive\u001b[0;34m(X, y, n_nonzero_coefs, tol, XTX)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Copy lower triangle to upper triangle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0msolutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcholesky_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mATy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# FINALLY, GET NEW RESIDUAL r=y-Ax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-30b01b536fcf>\u001b[0m in \u001b[0;36mcholesky_solve\u001b[0;34m(ATA, ATy)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mATy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mATy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mATy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 14.76 GiB total capacity; 11.02 GiB already allocated; 1.56 GiB free; 11.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XXhsEvzIjlZP",
        "mopvGI8tD6tB",
        "XzVT5tI_YWCI",
        "pa-ia6e8EPIr"
      ],
      "name": "BERT.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "166b1b75bc9c4da3870deca4d4451f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49b86b4abe264409aa0334b206d226ac",
              "IPY_MODEL_51cae743affe466492e567e468e7b5c4",
              "IPY_MODEL_09e53fc1789e470ea14bde6d4d4d3464"
            ],
            "layout": "IPY_MODEL_666fbffb859c4d8e9deb05d9dfd8e50b"
          }
        },
        "49b86b4abe264409aa0334b206d226ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_888f176bc0fa44b9aca9517081c75f8e",
            "placeholder": "​",
            "style": "IPY_MODEL_e65132124d1b4bcbb8ed64d5b7abe923",
            "value": "Downloading: 100%"
          }
        },
        "51cae743affe466492e567e468e7b5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ad83f1c594042089fce60f63b4c292d",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_459f4b02bd804165ace138f8170fbdf9",
            "value": 231508
          }
        },
        "09e53fc1789e470ea14bde6d4d4d3464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d96a8dd9e0b84737929d35c648153b31",
            "placeholder": "​",
            "style": "IPY_MODEL_6d7ce69aca284e2f977b3e2e80239462",
            "value": " 226k/226k [00:00&lt;00:00, 287kB/s]"
          }
        },
        "666fbffb859c4d8e9deb05d9dfd8e50b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888f176bc0fa44b9aca9517081c75f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e65132124d1b4bcbb8ed64d5b7abe923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ad83f1c594042089fce60f63b4c292d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "459f4b02bd804165ace138f8170fbdf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d96a8dd9e0b84737929d35c648153b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d7ce69aca284e2f977b3e2e80239462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "970cefb5300b4005a3557d1f25daacc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f80796b54f8c4a6a81408ce977c2266a",
              "IPY_MODEL_37938b0caba9490f92762c9658a0d785",
              "IPY_MODEL_5eac9a4c1cd24d8db26a3f1cbadf72a1"
            ],
            "layout": "IPY_MODEL_f030bca31dfa42e7abbcb640944ccb66"
          }
        },
        "f80796b54f8c4a6a81408ce977c2266a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ded8adf558494410b42ed733f4820acb",
            "placeholder": "​",
            "style": "IPY_MODEL_0f6366a8a68a4533a20400535d19009d",
            "value": "Downloading: 100%"
          }
        },
        "37938b0caba9490f92762c9658a0d785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ca03612b6ea45d4bc630f26894caa17",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57f18d4ca11248daa7d5c5954e80e5f0",
            "value": 28
          }
        },
        "5eac9a4c1cd24d8db26a3f1cbadf72a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0506499e3197427abdc87a0aefa20119",
            "placeholder": "​",
            "style": "IPY_MODEL_3184bad1dc2e43f1a0f9fb3c24402b43",
            "value": " 28.0/28.0 [00:00&lt;00:00, 852B/s]"
          }
        },
        "f030bca31dfa42e7abbcb640944ccb66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ded8adf558494410b42ed733f4820acb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f6366a8a68a4533a20400535d19009d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ca03612b6ea45d4bc630f26894caa17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57f18d4ca11248daa7d5c5954e80e5f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0506499e3197427abdc87a0aefa20119": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3184bad1dc2e43f1a0f9fb3c24402b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9289c171a6a46e9a64352cebe6ecaab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fd11ef7b9304497b0bff839b270c209",
              "IPY_MODEL_c4ed52c18bdb4bbfad24fd7bf9e25a12",
              "IPY_MODEL_b75f16a0d7564a43bc9e789cbfac7ed6"
            ],
            "layout": "IPY_MODEL_f9426698e6c94a76915a91f9ca14f23f"
          }
        },
        "9fd11ef7b9304497b0bff839b270c209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb2ff1fc65a24a8dbf62537c23e3b9b9",
            "placeholder": "​",
            "style": "IPY_MODEL_a67915d419b94295b30b9efcf5fc2b96",
            "value": "Downloading: 100%"
          }
        },
        "c4ed52c18bdb4bbfad24fd7bf9e25a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43365a61508a46c589c184451af63601",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a275beec70d74529bda89704988ffd3c",
            "value": 570
          }
        },
        "b75f16a0d7564a43bc9e789cbfac7ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61442e598a0f40de89c1c71a1ba3c3dc",
            "placeholder": "​",
            "style": "IPY_MODEL_5c8d9dc61a2f47e58ab01b504d0415c7",
            "value": " 570/570 [00:00&lt;00:00, 6.89kB/s]"
          }
        },
        "f9426698e6c94a76915a91f9ca14f23f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb2ff1fc65a24a8dbf62537c23e3b9b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a67915d419b94295b30b9efcf5fc2b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43365a61508a46c589c184451af63601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a275beec70d74529bda89704988ffd3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61442e598a0f40de89c1c71a1ba3c3dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c8d9dc61a2f47e58ab01b504d0415c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}